<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Optical Flow | My New Hugo Site</title>
<meta name="keywords" content="CV, #Deep_Learning, Introduction">
<meta name="description" content="Optical flow quantifies the motion of objects between consecutive frames captured by a camera. These algorithms attempt to capture the apparent motion of brightness patterns in the image. It is an important subfield of computer vision, enabling machines to understand scene dynamics and movement.

What is brightness
Lets break down the definition of brightness. In an image, brightness refers to the intensity of light at each pixel. It determines how light or dark a pixel appears.">
<meta name="author" content="">
<link rel="canonical" href="https://nguyentuss.github.io/posts/optical-flow/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.6da9a63d25a9608bca2f7f907a030e887a7dd3c3f3918e4cc113129361414bda.css" integrity="sha256-bammPSWpYIvKL3&#43;QegMOiHp908PzkY5MwRMSk2FBS9o=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://nguyentuss.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://nguyentuss.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://nguyentuss.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://nguyentuss.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://nguyentuss.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://nguyentuss.github.io/posts/optical-flow/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          throwOnError : false
        });
    });
</script>


<meta property="og:url" content="https://nguyentuss.github.io/posts/optical-flow/">
  <meta property="og:site_name" content="My New Hugo Site">
  <meta property="og:title" content="Optical Flow">
  <meta property="og:description" content="Optical flow quantifies the motion of objects between consecutive frames captured by a camera. These algorithms attempt to capture the apparent motion of brightness patterns in the image. It is an important subfield of computer vision, enabling machines to understand scene dynamics and movement. What is brightness Lets break down the definition of brightness. In an image, brightness refers to the intensity of light at each pixel. It determines how light or dark a pixel appears.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-03-13T22:05:35+07:00">
    <meta property="article:modified_time" content="2025-03-13T22:05:35+07:00">
    <meta property="article:tag" content="CV">
    <meta property="article:tag" content="#Deep_Learning">
    <meta property="article:tag" content="Introduction">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Optical Flow">
<meta name="twitter:description" content="Optical flow quantifies the motion of objects between consecutive frames captured by a camera. These algorithms attempt to capture the apparent motion of brightness patterns in the image. It is an important subfield of computer vision, enabling machines to understand scene dynamics and movement.

What is brightness
Lets break down the definition of brightness. In an image, brightness refers to the intensity of light at each pixel. It determines how light or dark a pixel appears.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://nguyentuss.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Optical Flow",
      "item": "https://nguyentuss.github.io/posts/optical-flow/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Optical Flow",
  "name": "Optical Flow",
  "description": "Optical flow quantifies the motion of objects between consecutive frames captured by a camera. These algorithms attempt to capture the apparent motion of brightness patterns in the image. It is an important subfield of computer vision, enabling machines to understand scene dynamics and movement. What is brightness Lets break down the definition of brightness. In an image, brightness refers to the intensity of light at each pixel. It determines how light or dark a pixel appears.\n",
  "keywords": [
    "CV", "#Deep_Learning", "Introduction"
  ],
  "articleBody": "Optical flow quantifies the motion of objects between consecutive frames captured by a camera. These algorithms attempt to capture the apparent motion of brightness patterns in the image. It is an important subfield of computer vision, enabling machines to understand scene dynamics and movement. What is brightness Lets break down the definition of brightness. In an image, brightness refers to the intensity of light at each pixel. It determines how light or dark a pixel appears.\nHow Brightness Is Represented in Images Grayscale Images Each pixel has a single intensity value ranging from: 0 (black) 255 (white) Color Images (RGB Format) Each pixel is made of Red, Green, and Blue (RGB) channels. Each channel has values from 0 to 255. Brightness is the combined intensity of all three channels. How to Measure Brightness? For grayscale images: Brightness is just the pixel value. For color images: A common way to measure brightness is: $$ brightness = \\frac{R+B+G}{3} $$ Basic Gradient-Based Estimation A common starting point for optical flow estimation is to assume that pixel intensities are translated from one frame to next $$ I(\\vec{x},t)=I(\\vec{x}+\\vec{u},t+1) \\tag{1}$$ where $I(\\vec{x},t)$ is an image intensity as a function of space $\\vec{x}=(x,y)^T$ and time $t$, and $\\vec{u}=(u_1,u_2)^T$ is the 2D velocity. That equation only holds under brightness constancy assumption. Of course, brightness constancy rarely hold exactly. The underlying assumptions that surface radiance remains fixed from one frame to next. One can fabricate scenes for which this holds, the scene might be constrained to contain only Lambertian Surface (no secularities), with a distant point source (so that changing the distance to the light source has no effect), no object rotations, and no secondary illumination (shadows or inter-surface reflection). Although this is unrealistic, it is remarkable that the brightness constancy assumption works well in practice. To derive an estimator for 2D velocity $\\vec{u}$, we first consider the 1D case. Let $f_1(x)$ and $f_2(x)$ be 1D signals (images) at two time instants. In the figure above, suppose that $f_2(x)$ is a translated version $f_1(x)$, let $f_2(x)=f_1(x-d)$ where d denotes the translation. A Taylor series expansion of $f_1(x-d)$ about x is given by\n$$ f_1(x-d) = f_1(x) -df’_1(x) + \\frac{d^2}{2!}f’’1(x)-…= \\sum{0}^{\\infty}\\frac{(-d)^n}{n!}f^{(n)} \\tag{2}(x) $$\nWith this expansion, we can rewrite the difference between two signals at location $x$\n$$ f_1(x)-f_2(x) = df_1(x)-O(d^2f’’_1) \\tag{3} $$\nIgnoring the second- and higher-order terms, we obtain the approximation at $x$\n$$ \\hat{d}=\\frac{f_1(x)-f_2(x)}{f’_1(x)} \\tag{4} $$\nThe 1D case generalizes straightforwardly to 2D. Assume that the displaced image is well approximated y the first-order Taylor series:\n$$ I(\\vec{x}+\\vec{u},t+1)\\approx I(\\vec{x},t)+\\vec{u}\\nabla I(\\vec{x},t)+I_t(\\vec{x},t)\\tag{5}$$\nwhere $\\nabla I(\\vec{x},t)=(I_x,I_y)$, which $I_x,I_y$ show how brightness changes in the x and y directions and $I_t$ show how brightness changes over time of image $I(\\vec{x},t)$ at time $t$. If $I_t\u003e0$, the pixel is getting brighter and $I_t\u003c0$ otherwise, and $\\vec{u}=(u_1,u_2)^T$ is the 2D velocity. Ignoring the high-order Taylor series, substitute with equation $(1)$, we obtain\n$$\\nabla I(\\vec{x},t)\\vec{u}+I_t(\\vec{x},t)=0 \\tag{6}$$ This equation relates the velocity to the space-time image derivatives at one image location, and is often called the gradient constraint equation or Optical flow constraint equation(OFCE). If one has access to only two frames, or cannot estimate $I_t$, it is straight-forward to derive a closely related gradient constraint, in which $I_t(\\vec{x},t)$ is replaced by $\\delta I(\\vec{x},t) \\triangleq I(\\vec{x},t+1)-I(\\vec{x},t)$\nIntensity Conversation Imagine you are watching a video where a white ball moves across a dark background. As the ball moves, its brightness (intensity) remains unchanged, meaning the white color of the ball does not fade or darken while it moves. Let\n$I(x,y,t)$ represent the brightness (grayscale intensity) at position $(x,y)$ at time $t$. The ball moves along a path $\\vec{x}=(x(t),y(t))$. The intensity of the ball (its brightness) remains constant at $c$. From the equation $$ I(\\vec{x}(t),t)=c \\tag{7} $$ This means that any time t, the intensity in the track remain the same, regardless of its positions. The temporal derivative of which yields $$ \\frac{d}{dt}I(\\vec{x}(t),t)=0 \\tag{8} $$ Expanding the left-hand-side, using chain rule give us $$ \\frac{d}{dt}I(\\vec{x}(t),t)=\\frac{\\partial I}{\\partial x}\\frac{\\partial x}{\\partial t} + \\frac{\\partial I}{\\partial y}\\frac{\\partial y}{\\partial t} + \\frac{\\partial I}{\\partial t}\\frac{\\partial t}{\\partial t} = \\nabla I(\\vec{x},t)\\vec{u} + I_t \\tag{9} $$ where the path derivative it just the optical flow $\\vec{u} \\triangleq (dx/dt,dy/dt)^T$. Combine $(8),(9)$ we have gradient constraint equation. Least-Squares Estimation Once cannot recover u from one gradient constraint since $(6)$ is one equation with two unknown, $u_1$ and $u_2$. The intensity gradient constrains the flow to a one parameter family of velocities along a line in velocity space. Two vectors are perpendicular if their dot product is zero:\n$$ \\nabla I \\cdot \\vec{d} = 0 $$\nwhere:\n$\\nabla I = (I_x, I_y)$ (gradient direction), and $\\vec{d}$ is the direction vector of the constraint line. From the equation $I_x , u + I_y , v = -I_t$ we can rewrite the direction vector of the line as: $$ \\vec{d} = (I_y, -I_x) $$ Now, compute the dot product:\n$$ (I_x, I_y) \\cdot (I_y, -I_x) = I_x I_y + I_y (-I_x) = I_x I_y - I_x I_y = 0 $$ So that the optical flow constraint line is perpendicular to $\\nabla I$ and its perpendicular distance from the origins $|I_t|/||\\nabla I||$. One common way to further constrain $\\vec{u}$ is to use the gradient constraints, assuming they share the same 2D velocity. With many constraints there may be no velocity that simultaneously satisfies them all, so instead we find the velocity that minimizes the constraint errors. The least-squares (LS) estimator minimizes the squared errors $$ E(\\mathbf{u}) ;=; \\sum_{\\vec{x}} g(\\vec{x}) \\Bigl[;\\vec{u} ,\\cdot, \\nabla I(\\vec{x}, t) ;+; I_t(\\vec{x}, t)\\Bigr]^{2} $$\nwhere $g(\\vec{x})$ is a weighting function that determines the support of the estimator (the region within which we combine constraints). It is common to let $g(\\vec{x})$ be Gaussian in order to weight constraints in the center of the neighborhood more highly, giving them more influence. The 2D velocity $\\hat{u}$ that minimizes $E(\\vec{u})$ is the least squares flow estimate. The minimum of $E(\\vec{u})$ can be found from its critical points, where its derivatives with respect to $\\vec{u}$ are zero; i.e.,\n$$ \\frac{\\partial E(u_1, u_2)}{\\partial u_1}=\\sum_{\\vec{x}} g(\\tilde{x})\\Bigl[u_1 ,I_x^2+ u_2 , I_x , I_y+ I_t , I_x\\Bigr]= 0 $$\n$$ \\frac{\\partial E(u_1, u_2)}{\\partial u_2} =\\sum_{\\vec{x}} g(\\vec{x})\\Bigl[u_1 , I_xI_y + u_2 , I_y^2+ I_t , I_y\\Bigr]= 0 $$ These equations can we rewritten in matrix form\n$$ \\mathbf{M}\\vec{u}=\\vec b $$ Where $\\mathbf{M}$ and $\\vec{b}$ are $$ \\mathbf{M} = \\begin{bmatrix} \\sum g I_x^2 \u0026 \\sum g I_xI_y \\ \\sum gI_y^2 \u0026 \\sum g I_x I_y \\end{bmatrix} , \\vec{b} = - \\begin{pmatrix} I_tI_x \\ I_tI_y \\end{pmatrix} $$ When $\\mathbf{M}$ has rank 2, then the LS estimate is $\\hat{u}=\\mathbf{M}^{-1}\\vec{b}$\nFind root Back to the equation OCFE, this gives you one equation but find two unknowns $(u,v)$, this means the solution cannot be determined uniquely with a single constraint (a single pixel), We need more information to determine the correct motion for each pixel. So there are different methods handle this problem in different ways\nLucas-Kanade Optical Flow (Local Estimation) Finds $(u,v)$ in small patches (instead of a single pixel). Assumes motion is constant within a small neighborhood. Uses least-squares optimization to find $(u,v)$ for the whole patch. Horn-Schunck Optical Flow (Global Estimation) Finds $(u,v)$ for every pixel across the entire image. Uses a smoothness constraint so that neighboring pixels have similar motion. Solves $(u,v)$ as a global optimization problem. Click this link to see more informationand this Brox Optical Flow \u0026 Deep Learning-Based Methods Use more complex constraints (e.g., gradient constancy, deep learning models). Improve accuracy for large and complex motions Practice Reference Video Analysis Algorithms in Computer Vision Thuật toán phân tích video trong thị giác máy tính – VinBigdata Product Optical Flow Estimation | Papers With Code (recommend) 14.3 OF - HornSchunck ",
  "wordCount" : "1274",
  "inLanguage": "en",
  "datePublished": "2025-03-13T22:05:35+07:00",
  "dateModified": "2025-03-13T22:05:35+07:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://nguyentuss.github.io/posts/optical-flow/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "My New Hugo Site",
    "logo": {
      "@type": "ImageObject",
      "url": "https://nguyentuss.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class=" dark" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://nguyentuss.github.io/" accesskey="h" title="My New Hugo Site (Alt + H)">My New Hugo Site</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Optical Flow
    </h1>
    <div class="post-meta"><span title='2025-03-13 22:05:35 +0700 +07'>March 13, 2025</span>

</div>
  </header> 
  <div class="post-content"><p>Optical flow quantifies the motion of objects between consecutive frames captured by a camera. These algorithms attempt to capture the apparent motion of brightness patterns in the image. It is an important subfield of computer vision, enabling machines to understand scene dynamics and movement.
<img loading="lazy" src="/posts/optical-flow/img/pic1.png"></p>
<h2 id="what-is-brightness">What is brightness<a hidden class="anchor" aria-hidden="true" href="#what-is-brightness">#</a></h2>
<p>Lets break down the definition of brightness. In an image, <strong>brightness</strong> refers to the intensity of light at each pixel. It determines how <strong>light or dark</strong> a pixel appears.</p>
<h3 id="how-brightness-is-represented-in-images">How Brightness Is Represented in Images<a hidden class="anchor" aria-hidden="true" href="#how-brightness-is-represented-in-images">#</a></h3>
<h4 id="grayscale-images">Grayscale Images<a hidden class="anchor" aria-hidden="true" href="#grayscale-images">#</a></h4>
<ul>
<li>Each pixel has a single intensity value ranging from:
<ul>
<li><strong>0 (black)</strong></li>
<li><strong>255 (white)</strong></li>
</ul>
</li>
</ul>
<h4 id="color-images-rgb-format">Color Images (RGB Format)<a hidden class="anchor" aria-hidden="true" href="#color-images-rgb-format">#</a></h4>
<ul>
<li>Each pixel is made of <strong>Red, Green, and Blue (RGB)</strong> channels.</li>
<li>Each channel has values from <strong>0 to 255</strong>.</li>
<li><strong>Brightness is the combined intensity of all three channels</strong>.</li>
</ul>
<h4 id="how-to-measure-brightness">How to Measure Brightness?<a hidden class="anchor" aria-hidden="true" href="#how-to-measure-brightness">#</a></h4>
<ul>
<li><strong>For grayscale images:</strong> Brightness is just the pixel value.</li>
<li><strong>For color images:</strong> A common way to measure brightness is:
$$
brightness = \frac{R+B+G}{3}
$$</li>
</ul>
<h2 id="basic-gradient-based-estimation">Basic Gradient-Based Estimation<a hidden class="anchor" aria-hidden="true" href="#basic-gradient-based-estimation">#</a></h2>
<p>A common starting point for optical flow estimation is to assume that pixel intensities are translated from one frame to next
$$ I(\vec{x},t)=I(\vec{x}+\vec{u},t+1) \tag{1}$$
where $I(\vec{x},t)$ is an image intensity as a function of space $\vec{x}=(x,y)^T$ and time $t$, and $\vec{u}=(u_1,u_2)^T$ is the 2D velocity.
That equation only holds under <em>brightness constancy</em> assumption. Of course, <em>brightness constancy</em> rarely hold exactly. The underlying assumptions that surface radiance remains fixed from one frame to next. One can fabricate scenes for which this holds, the scene might be constrained to contain only <a href="https://www.azooptics.com/Article.aspx?ArticleID=790">Lambertian Surface</a> (no secularities), with a distant point source (so that changing the distance to the light source has no effect), no object rotations, and no secondary illumination (shadows or inter-surface reflection). Although this is unrealistic, it is remarkable that the brightness constancy assumption works well in practice.
<img loading="lazy" src="/posts/optical-flow/img/pic2.png">
To derive an estimator for 2D velocity $\vec{u}$, we first consider the 1D case. Let $f_1(x)$ and $f_2(x)$ be 1D signals (images) at two time instants. In the figure above, suppose that $f_2(x)$ is a translated version $f_1(x)$, let $f_2(x)=f_1(x-d)$ where <em>d</em> denotes the translation. <a href="https://tutorial.math.lamar.edu/classes/calcii/taylorseries.aspx">A Taylor series expansion</a> of $f_1(x-d)$ about x is given by</p>
<p>$$
f_1(x-d) = f_1(x) -df&rsquo;_1(x) + \frac{d^2}{2!}f&rsquo;&rsquo;<em>1(x)-&hellip;= \sum</em>{0}^{\infty}\frac{(-d)^n}{n!}f^{(n)} \tag{2}(x)
$$</p>
<p>With this expansion, we can rewrite the difference between two signals at location $x$</p>
<p>$$
f_1(x)-f_2(x) = df_1(x)-O(d^2f&rsquo;&rsquo;_1) \tag{3}
$$</p>
<p>Ignoring the second- and higher-order terms, we obtain the approximation at $x$</p>
<p>$$
\hat{d}=\frac{f_1(x)-f_2(x)}{f&rsquo;_1(x)} \tag{4}
$$</p>
<p>The 1D case generalizes straightforwardly to 2D. Assume that the displaced image is well approximated y the first-order Taylor series:</p>
<p>$$ I(\vec{x}+\vec{u},t+1)\approx I(\vec{x},t)+\vec{u}\nabla I(\vec{x},t)+I_t(\vec{x},t)\tag{5}$$</p>
<p>where $\nabla I(\vec{x},t)=(I_x,I_y)$, which $I_x,I_y$ show how brightness changes in the x and y directions and $I_t$ show how brightness changes over time of image $I(\vec{x},t)$ at time $t$. If $I_t&gt;0$, the pixel is getting brighter and $I_t&lt;0$ otherwise, and $\vec{u}=(u_1,u_2)^T$ is the 2D velocity.
Ignoring the high-order Taylor series, substitute with equation $(1)$, we obtain</p>
<p>$$\nabla I(\vec{x},t)\vec{u}+I_t(\vec{x},t)=0 \tag{6}$$
This equation relates the velocity to the space-time image derivatives at one image location, and is often called the <em>gradient constraint equation</em> or <strong>Optical flow constraint equation(OFCE)</strong>. If one has access to only two frames, or cannot estimate $I_t$, it is straight-forward to derive a closely related gradient constraint, in which $I_t(\vec{x},t)$ is replaced by $\delta I(\vec{x},t) \triangleq I(\vec{x},t+1)-I(\vec{x},t)$</p>
<h2 id="intensity-conversation">Intensity Conversation<a hidden class="anchor" aria-hidden="true" href="#intensity-conversation">#</a></h2>
<!-- raw HTML omitted -->
<p>Imagine you are watching a video where a <strong>white ball moves across a dark background</strong>. As the ball moves, its <strong>brightness (intensity) remains unchanged</strong>, meaning the white color of the ball does not fade or darken while it moves.
Let</p>
<ul>
<li>$I(x,y,t)$ represent the brightness (grayscale intensity) at position $(x,y)$ at time $t$.</li>
<li>The ball moves along a path $\vec{x}=(x(t),y(t))$.</li>
<li>The intensity of the ball (its brightness) remains constant at $c$.
From the equation
$$
I(\vec{x}(t),t)=c \tag{7}
$$
This means that any time t, the intensity in the track remain the same, regardless of its positions. The temporal derivative of which yields
$$
\frac{d}{dt}I(\vec{x}(t),t)=0 \tag{8}
$$
Expanding the left-hand-side, using chain rule give us
$$
\frac{d}{dt}I(\vec{x}(t),t)=\frac{\partial I}{\partial x}\frac{\partial x}{\partial t} + \frac{\partial I}{\partial y}\frac{\partial y}{\partial t} + \frac{\partial I}{\partial t}\frac{\partial t}{\partial t} = \nabla I(\vec{x},t)\vec{u} + I_t \tag{9}
$$
where the path derivative it just the optical flow $\vec{u} \triangleq (dx/dt,dy/dt)^T$. Combine $(8),(9)$ we have <em>gradient constraint equation</em>.</li>
</ul>
<h2 id="least-squares-estimation">Least-Squares Estimation<a hidden class="anchor" aria-hidden="true" href="#least-squares-estimation">#</a></h2>
<p>Once cannot recover u from one gradient constraint since $(6)$ is one equation with two unknown, $u_1$ and $u_2$. The intensity gradient constrains the flow to a one parameter family of velocities along a line in <em>velocity space</em>.
Two vectors are perpendicular if their dot product is zero:</p>
<p>$$
\nabla I \cdot \vec{d} = 0
$$</p>
<p>where:</p>
<ul>
<li>$\nabla I = (I_x, I_y)$ <em>(gradient direction)</em>, and</li>
<li>$\vec{d}$ is the <strong>direction vector of the constraint line</strong>.</li>
</ul>
<p>From the equation $I_x , u + I_y , v = -I_t$ we can rewrite the direction vector of the line as:
$$
\vec{d} = (I_y, -I_x)
$$
Now, compute the dot product:</p>
<p>$$
(I_x, I_y) \cdot (I_y, -I_x)
= I_x I_y + I_y (-I_x)
= I_x I_y - I_x I_y
= 0
$$
So that the optical flow constraint line is perpendicular to $\nabla I$ and its perpendicular distance from the origins $|I_t|/||\nabla I||$.
One common way to further constrain $\vec{u}$ is to use the gradient constraints, assuming they share the same 2D velocity. With many constraints there may be no velocity that simultaneously satisfies them all, so instead we find the velocity that minimizes the constraint errors. The least-squares (LS) estimator minimizes the squared errors
$$
E(\mathbf{u}) ;=; \sum_{\vec{x}} g(\vec{x})
\Bigl[;\vec{u} ,\cdot, \nabla I(\vec{x}, t) ;+; I_t(\vec{x}, t)\Bigr]^{2}
$$</p>
<p>where $g(\vec{x})$ is a weighting function that determines the <em>support</em> of the estimator (the region within which we combine constraints). It is common to let $g(\vec{x})$ be Gaussian in order to weight constraints in the center of the neighborhood more highly, giving them more influence. The 2D velocity $\hat{u}$ that minimizes $E(\vec{u})$ is the least squares flow estimate.
The minimum of $E(\vec{u})$ can be found from its critical points, where its derivatives with respect to $\vec{u}$ are zero; i.e.,</p>
<p>$$
\frac{\partial E(u_1, u_2)}{\partial u_1}=\sum_{\vec{x}} g(\tilde{x})\Bigl[u_1 ,I_x^2+ u_2 , I_x , I_y+ I_t , I_x\Bigr]= 0
$$</p>
<p>$$
\frac{\partial E(u_1, u_2)}{\partial u_2}
=\sum_{\vec{x}} g(\vec{x})\Bigl[u_1 , I_xI_y + u_2 , I_y^2+ I_t , I_y\Bigr]= 0
$$
These equations can we rewritten in matrix form</p>
<p>$$
\mathbf{M}\vec{u}=\vec b
$$
Where $\mathbf{M}$ and $\vec{b}$ are
$$
\mathbf{M} =
\begin{bmatrix}
\sum g I_x^2 &amp; \sum g I_xI_y \
\sum gI_y^2 &amp; \sum g I_x I_y
\end{bmatrix}
, \vec{b} = -
\begin{pmatrix}
I_tI_x \
I_tI_y
\end{pmatrix}
$$
When $\mathbf{M}$ has rank 2, then the LS estimate is $\hat{u}=\mathbf{M}^{-1}\vec{b}$</p>
<h2 id="find-root">Find root<a hidden class="anchor" aria-hidden="true" href="#find-root">#</a></h2>
<p>Back to the equation <strong>OCFE</strong>, this gives you one equation but find two unknowns $(u,v)$, this means the solution cannot be determined uniquely with a single constraint (a single pixel), We <strong>need more information</strong> to determine the correct motion for each pixel. So there are different methods handle this problem in different ways</p>
<h3 id="lucas-kanade-optical-flow-local-estimation">Lucas-Kanade Optical Flow (Local Estimation)<a hidden class="anchor" aria-hidden="true" href="#lucas-kanade-optical-flow-local-estimation">#</a></h3>
<ul>
<li>Finds $(u,v)$ in small patches (instead of a single pixel).</li>
<li>Assumes motion is  <strong>constant within a small neighborhood</strong>.</li>
<li>Uses <strong>least-squares optimization</strong> to find $(u,v)$ for the whole patch.</li>
</ul>
<h3 id="horn-schunck-optical-flow-global-estimation">Horn-Schunck Optical Flow (Global Estimation)<a hidden class="anchor" aria-hidden="true" href="#horn-schunck-optical-flow-global-estimation">#</a></h3>
<ul>
<li>Finds $(u,v)$ for every pixel across the entire image.</li>
<li>Uses a <strong>smoothness constraint</strong> so that neighboring pixels have similar motion.</li>
<li>Solves $(u,v)$ as a global optimization problem.
<a href="https://www.ipol.im/pub/art/2013/20/article_lr.pdf">Click this link to see more information</a>and <a href="https://www.cs.cmu.edu/~16385/s17/Slides/14.3_OF__HornSchunck.pdf">this</a></li>
</ul>
<h3 id="brox-optical-flow--deep-learning-based-methods">Brox Optical Flow &amp; Deep Learning-Based Methods<a hidden class="anchor" aria-hidden="true" href="#brox-optical-flow--deep-learning-based-methods">#</a></h3>
<ul>
<li>Use <strong>more complex constraints</strong> (e.g., gradient constancy, deep learning models).</li>
<li>Improve accuracy for <strong>large and complex motions</strong></li>
</ul>
<h2 id="practice">Practice<a hidden class="anchor" aria-hidden="true" href="#practice">#</a></h2>
<h2 id="reference">Reference<a hidden class="anchor" aria-hidden="true" href="#reference">#</a></h2>
<ol>
<li><a href="https://www.thinkautonomous.ai/blog/computer-vision-from-image-to-video-analysis/">Video Analysis Algorithms in Computer Vision</a></li>
<li><a href="https://product.vinbigdata.org/thuat-toan-phan-tich-video-trong-thi-giac-may-tinh/">Thuật toán phân tích video trong thị giác máy tính – VinBigdata Product</a></li>
<li><a href="https://paperswithcode.com/task/optical-flow-estimation#task-home">Optical Flow Estimation | Papers With Code</a> (recommend)</li>
<li><a href="https://www.cs.cmu.edu/~16385/s17/Slides/14.3_OF__HornSchunck.pdf">14.3 OF - HornSchunck</a></li>
</ol>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://nguyentuss.github.io/tags/cv/">CV</a></li>
      <li><a href="https://nguyentuss.github.io/tags/%23deep_learning/">#Deep_Learning</a></li>
      <li><a href="https://nguyentuss.github.io/tags/introduction/">Introduction</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://nguyentuss.github.io/">My New Hugo Site</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
