[{"content":"My name is Nguyen Ngoc Tu, and I do research in AI, focusing on Computer Vision. I am a research fellow at CVIP, where I work on improving how AI understands images and videos. I enjoy exploring new ideas and finding ways to make AI more useful in real life.\n","permalink":"http://localhost:1313/posts/about/","summary":"\u003cp\u003eMy name is \u003cstrong\u003eNguyen Ngoc Tu\u003c/strong\u003e, and I do research in \u003cstrong\u003eAI\u003c/strong\u003e, focusing on \u003cstrong\u003eComputer Vision\u003c/strong\u003e. I am a \u003cstrong\u003eresearch fellow at CVIP\u003c/strong\u003e, where I work on improving how AI understands images and videos. I enjoy exploring new ideas and finding ways to make AI more useful in real life.\u003c/p\u003e","title":"About me"},{"content":"Introduction In Machine Learning, the core problem is that solving the parameter estimation (model fitting), we want to find the values for a set of variable $\\theta\\in\\Theta$, that minimized the scalar loss function or cost function $\\mathcal{L}(\\theta) \\rightarrow \\mathbb{R}$. This is called a optimization problem. $$ \\theta^{opt} \\in \\arg\\min \\mathcal{L}(\\theta) $$ We will assume that the parameter space is given by $\\Theta \\subseteq \\mathbb{R}^D$, where $D$ is the number of variables being optimized over.\nIf we want to maximize a score function $R(\\theta)$, we can minimize the loss function $\\mathcal{L}(\\theta)=-R(\\theta)$, we will use this term objective function to determine a function we want to minimize or maximize. An algorithm that can find an optimum of an objective function is often called a solver.\nLocal and Global Optimization A point satisfies the optimization problem is called a global optimum. Finding such a point is called global optimization. In general, finding global optimum is very hard to compute. In such cases we will find the local optimum.\nA point $x^{opt}$ is called a local minimum if: $$ \\exists \\varepsilon \u0026gt; 0 \\text{ such that } f(x^{opt} ) \\leq f(x), \\quad \\forall x \\text{ with } |x - x^{opt} | \u0026lt; \\varepsilon. $$\nA point $x^{opt}$ is a local maximum if: $$ \\exists \\varepsilon \u0026gt; 0 \\text{ such that } f(x^{opt} ) \\geq f(x), \\quad \\forall x \\text{ with } |x - x^{opt} | \u0026lt; \\varepsilon. $$\nOptimality conditions for local and global optimum For continuous, twice differentiable functions, we can precisely characterize the points which correspond to local minima. Let $g(\\theta) = \\nabla\\mathcal{L}(\\theta)$ be the gradient vector, and $H(\\theta)=\\nabla^2 \\mathcal{L}(\\theta)$ be the Hessian matrix. Let $g^{}=g({\\theta}^{opt})$ be the gradient of that point and $H^{opt}=H(\\theta^{opt})$ be the corresponding Hessian. We can show that the following\nNecessary condition: If $\\theta^{opt}$ is a local minimum, then $g^{opt}=0$ and $H^{opt}$ must be positive semi-difinite Sufficient condition: If $g^{opt}=0$ and $H^{opt}$ is positive definite, then $\\theta^{opt}$ is a local minimum. To see why the first condition is necessary, suppose we were at a point $\\theta^*$ at which the gradient is non-zero, we could decrease the function by following the negative gradient a small distance, this can make the gradient become zero, but would not be the optimal. Note that the stationary point could be local minimum, local maximum or saddle point, which is a point where some directions point downhill, and some uphill. More precisely, at a saddle point, the eigenvalues of the Hessian will be both positive and negative. However, if the Hessian at a point is positive semi-definite, then some directions may point uphill, while others are flat. Moreover, if the Hessian is strictly positive definite, then we are at the bottom of a “bowl”, and all directions point uphill, which is sufficient for this to be a minimum. Constrained and unconstrained optimization Lagrange multipliers Lagrange multipliers, also sometimes called undermined multipliers, are used to find the stationary points of a function of several variables subject to one or more constraints. Consider the problem of finding the maximum of a function $f(x_1,x_2)$ subject to a constraint relating $x_1$ and $x_2$, which we write in the form $$ g(x_1,x_2)=0 $$ One approach would be to solve the constraint equation and thus express $x_2$ as a function of $x_1$ in the form $x_2=h(x_1)$. This can then be substituted into $f(x_1,x_2)$ to give a function of $x_1$ alone of the form $f(x_1, h(x_1))$. The maximum with respect to $x_1$ could then be found by differentiation in the usual way, to give the stationary value $\\hat{x}_1$(are found where the gradient is zero), which the corresponding value of $x_2$, $\\hat x_2 = h(\\hat x_1)$.\nOne problem with this approach is that it may be difficult to find an analytic solution of the constraint equation that allow $x_2$ to be expressed as an explicit function of $x_1$. Also, this approach treats $x_1$ and $x_2$ differently and so spoils the natural symmetry between these variables, it means that $x_2$ will depend on $x_1$ and make the function asymmetry into the problem that wasn\u0026rsquo;t there originally. The symmetry in here means that the problem treats both variables equally, none of those to be more important than other. Breaking the symmetry can make the math messier, limit the generality of the solution\nA more elegant, and often simpler, approach is have the $\\lambda$ called a Lagrange multiplier. We shall motivate this technique from a geometrical perspective. Consider a D-dimensional variable $\\mathbf{x}$ with the components $x_1,\u0026hellip;,x_D$. The constraint equation $g(\\mathbf{x})=0$ then represents a (D-1)-dimensional surface in x-space as indicated in this image\nWe first note that at any point on the constraint surface the gradient $\\nabla g(\\mathbf{x})$ of the constraint function will be orthogonal to the surface. To see this, consider a point $\\mathbf{x}$ that lies on the constraints surface, and consider a nearby point $\\mathbf{x} + \\epsilon$ that also lies on the surface. If we make a Taylor expansion around $\\mathbf{x}$, we have $$ g(\\mathbf{x}+\\epsilon) \\simeq g(\\mathbf{x})+\\epsilon^T \\nabla g(\\mathbf{x}) $$ Because both $\\mathbf{x}$ and $\\mathbf{x}+\\epsilon$ lie on the constraint surface, we have $g(\\mathbf{x})=g(\\mathbf{x}+\\epsilon)$ and hence $\\epsilon^T \\nabla g(\\mathbf{x}) \\simeq 0$. In the limit $||\\epsilon|| \\rightarrow 0$ we have $\\epsilon^T \\nabla g(\\mathbf{x}) = 0$, and because $\\epsilon$ is then parallel to the constraint surface $g(\\mathbf{x})=0$, we see that the vector $\\nabla g$ is normal to the surface.\nNext we seek a point $\\hat{x}$ on the constraint surface such that $f(\\mathbf{x})$ is maximized. Such a point must have the property that the vector $\\nabla f(\\mathbf{x})$ is also orthogonal to the constraint surface, as illustrated in the figure above, because otherwise, we could increase the value $f(\\mathbf{x})$ by moving a short distance along the constraint surface. Thus, $\\nabla f$ and $\\nabla g$ are parallel (or anti-parallel) vectors, and so there must exist a parameter $\\lambda$ such that\n$$ \\nabla f+\\lambda \\nabla g=0 $$\nwhere $\\lambda \\neq 0$ is known as a Lagrange multiplier. Note that $\\lambda$ can have either sign. At this point, the Lagrange function is defined by\n$$ L(\\mathbf{x}, \\lambda) \\equiv f(\\mathbf{x}) + \\lambda g(\\mathbf{x}) $$ The constrained stationary condition is obtained by setting $\\nabla_\\mathbf{x} L = 0$. The condition $\\partial L/ \\partial\\lambda=0$ leads to the constraint equation $g(\\mathbf{x})=0$.\nThus, to find the maximum of a function $f(\\mathbf{x})$ subject to the constraint $g(\\mathbf{x})=0$, we define the Lagrangian with respect to both $\\mathbf{x}$ and $\\lambda$. For a D-dimensional vector $\\mathbf{x}$, this give D+1 equations that determine both the stationary point $\\widehat x$ and the value of $\\lambda$. If we are only interested in $\\widehat x$, then we can eliminate $\\lambda$ from the stationary equations without needing to find its value.\n$$ \\left{ \\begin{aligned} -2x_1 + \\lambda \u0026amp;= 0 \\ -2x_2 + \\lambda \u0026amp;=0 \\ x_1 + x_2 - 1 \u0026amp;= 0 \\end{aligned} \\right. $$\nWe can also consider the problem of maximum with inequality constraint of the form $g(\\mathbf{x})\\geq 0$\n","permalink":"http://localhost:1313/posts/optimization/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn \u003cem\u003eMachine Learning\u003c/em\u003e, the core problem is that solving the parameter estimation (model fitting), we want to find the values for a set of variable $\\theta\\in\\Theta$, that minimized the scalar \u003cstrong\u003eloss function\u003c/strong\u003e or \u003cstrong\u003ecost function\u003c/strong\u003e $\\mathcal{L}(\\theta) \\rightarrow \\mathbb{R}$. This is called a \u003cstrong\u003eoptimization problem\u003c/strong\u003e.\n$$\n\\theta^{opt} \\in \\arg\\min \\mathcal{L}(\\theta)\n$$\nWe will assume that the \u003cem\u003eparameter space\u003c/em\u003e is given by $\\Theta \\subseteq \\mathbb{R}^D$, where $D$ is the number of variables being optimized over.\u003c/p\u003e","title":"Optimization"},{"content":"Introduction In the section Univariate Models and Multivariate Models, we assumed all the parameters $\\theta$ is known. In this section, we discuss how to learn these parameters from data. The process of estimating $\\theta$ from $\\mathcal{D}$ is call model fitting, or training, and is at the heart of machine learning. There are many methods for producing such estimates, but most boil down to an optimization problem of the form. $$ \\widehat{\\theta} = \\arg\\min_{\\theta} \\mathcal{L}(\\theta) $$ where $\\mathcal{L(\\theta)}$ is some kind of loss function or objective function. We discuss several different loss functions in this chapter. In some cases we also discuss how to solve the optimization problem in closed form. In general, however we will need to use some kind of generic optimization algorithm, which we will discuss in Optimization. In addition to computing a point estimate $\\widehat{\\theta}$. We discuss how to model our uncertainty or confidence in this estimate. In statistics, the process of quantifying uncertainty about an unknown quantity estimated from a finite sample of data is called inference.\nMaximum likelihood estimation (MLE) The most common approach to parameter estimation is to pick the parameters that assign the highest probability to the training data; this is called maximum likelihood estimation or MLE. We give more details below, and then give a series of worked examples.\nDefinition We define the MLE as follows: $$ \\widehat{\\theta}{\\text{mle}} \\triangleq \\arg\\max{\\theta} p(\\mathcal{D} \\mid \\theta) $$\nWe usually assume the training examples are independently sampled from the same distribution, so the (conditional) likelihood becomes\n$$ p(\\mathcal{D} \\mid \\theta) = \\prod_{n=1}^{N} p(y_n \\mid x_n, \\theta) $$\nThis is known as the i.i.d assumption, which stands for “independent and identically distributed”. We usually work with the log likelihood, which is given by $$ \\ell(\\theta) \\triangleq \\log p(\\mathcal{D} \\mid \\theta) = \\sum_{n=1}^{N} \\log p(y_n \\mid x_n, \\theta) $$\nThis decomposes into a sum of terms, one per example. Thus, the MLE is given by $$ \\widehat{\\theta}{\\text{mle}} = \\arg\\max{\\theta} \\sum_{n=1}^{N} \\log p(y_n \\mid x_n, \\theta) $$\nSince most optimization algorithms (such as those discussed in Optimization) are designed to minimize cost functions, we can redefine the objective function to be the (conditional) negative log likelihood or NLL:\n$$ \\text{NLL}(\\theta) \\triangleq -\\log p(\\mathcal{D} \\mid \\theta) = -\\sum_{n=1}^{N} \\log p(y_n \\mid x_n, \\theta) $$ Minimizing this will give the MLE. If the model is unconditional (unsupervised), the MLE becomes\n$$ \\widehat{\\theta}{\\text{mle}} = \\arg\\min{\\theta} -\\sum_{n=1}^{N} \\log p(y_n) $$ since we have outputs $y_n$ but no inputs $x_n$. In statistics, it is standard to use $y$ to represent variables whose generative distribution we choose to model, and use $x$ to represent exogenous inputs (coming from outside the system), which are given but not generated. Alternatively we may want to maximize the joint likelihood of inputs and outputs. The MLE in this case becomes\n$$ \\widehat{\\theta}{\\text{mle}} = \\arg\\min{\\theta} -\\sum_{n=1}^{N} \\log p(y_n, x_n \\mid \\theta) $$\nJustification for MLE There are several ways to justify the method of MLE. One way is to view it as simple point approximation to the Bayesian posterior $p(\\theta|\\mathcal{D})$ using a uniform prior (A uniform prior is a type of prior distribution used in Bayesian statistics, where we assume that all values within a certain range are equally likely before we observe any data). In particular, suppose we approximate the posterior by a delta function, $p(\\theta \\mid \\mathcal{D}) = \\delta(\\theta - \\widehat\\theta_{\\text{map}})$, where $\\widehat{\\theta}_{\\text{map}}$ is the posterior mode, given by\n$$ \\widehat{\\theta}{\\text{map}} = \\arg\\max{\\theta} \\log p(\\theta \\mid \\mathcal{D}) = \\arg\\max_{\\theta} \\log p(\\mathcal{D} \\mid \\theta) + \\log p(\\theta) $$\nIf we use a uniform prior, $p(\\theta) \\propto 1$, the MAP estimate becomes equal to the MLE, $\\widehat\\theta_{\\text{map}} = \\widehat\\theta_{\\text{mle}}$. Another way to justify the use of the MLE is that the resulting predictive distribution $p(y \\mid \\widehat\\theta_{\\text{mle}})$ is as close as possible (in a sense to be defined below) to the empirical distribution of the data. In the unconditional case, the empirical distribution is defined by\n$$ p_{\\mathcal{D}}(y) \\triangleq \\frac{1}{N} \\sum_{n=1}^{N} \\delta(y - y_n) $$\nWe see that the empirical distribution is a series of delta functions or “spikes” at the observed training points. We want to create a model whose distribution $q(y) = p(y \\mid \\theta)$ is similar to $p_{\\mathcal{D}}(y)$. A standard way to measure the (dis)similarity between probability distributions $p$ and $q$ is the Kullback-Leibler divergence, or KL divergence. We give the details in here, but in brief this is defined as\n$$ D_{\\text{KL}}(p \\parallel q) = \\sum_{y} p(y) \\log \\frac{p(y)}{q(y)} $$ $$ = \\sum_{y} p(y) \\log p(y) - \\sum_{y} p(y) \\log q(y) $$ $$ = \\underbrace{-\\mathbb{H}(p)}{\\text{entropy}} + \\underbrace{\\mathbb{H}{\\text{ce}}(p,q)}_{\\text{cross-entropy}} $$\nwhere $\\mathbb{H}(p)$ is the entropy of $p$ (Cross Entropy), and $\\mathbb H_{\\text{ce}}(p,q)$ is the cross-entropy of $p$ and $q$. One can show that $D_{\\text{KL}}(p \\parallel q) \\geq 0$, with equality if $p = q$. If we define $q(y) = p(y \\mid \\theta)$, and set $p(y) = p_{\\mathcal{D}}(y)$, then the KL divergence becomes\n$$ D_{\\text{KL}}(p \\parallel q) = \\sum_{y} \\left[ p_{\\mathcal{D}}(y) \\log p_{\\mathcal{D}}(y) - p_{\\mathcal{D}}(y) \\log q(y) \\right] $$ $$ = -\\mathbb{H}(p_{\\mathcal{D}}) - \\frac{1}{N} \\sum_{n=1}^{N} \\log p(y_n \\mid \\theta) $$ $$ = \\text{const} + \\text{NLL}(\\theta) $$\nThe first term is a constant which we can ignore, leaving just the NLL. Thus minimizing the KL is equivalent to minimizing the NLL which is equivalent to computing the MLE. We can generalize the above results to the supervised (conditional) setting by using the following empirical distribution:\n$$ p_{\\mathcal{D}}(x, y) = p_{\\mathcal{D}}(y \\mid x)p_{\\mathcal{D}}(x) = \\frac{1}{N} \\sum_{n=1}^{N} \\delta(x - x_n)\\delta(y - y_n) $$\nThe expected KL then becomes\n$$ \\mathbb{E}{p{\\mathcal{D}}(x)} \\left[ D_{\\text{KL}}\\left( p_{\\mathcal{D}}(Y \\mid x) \\parallel q(Y \\mid x) \\right) \\right] = \\sum_{x} p_{\\mathcal{D}}(x) \\left[ \\sum_{y} p_{\\mathcal{D}}(y \\mid x) \\log \\frac{p_{\\mathcal{D}}(y \\mid x)}{q(y \\mid x)} \\right] $$ $$ = \\text{const} - \\sum_{x, y} p_{\\mathcal{D}}(x, y) \\log q(y \\mid x) $$ $$ = \\text{const} - \\frac{1}{N} \\sum_{n=1}^{N} \\log p(y_n \\mid x_n, \\theta) $$\nMinimizing this is equivalent to minimizing the conditional NLL in Equation.\nExample: MLE for the Bernoulli distribution Suppose $Y$ is a random variable representing a coin toss where the event $Y=1$ corresponds to heads and $Y=0$ corresponds to tails. Let $\\theta=p(Y=1)$ be the probability of heads. The probability distribution for this r.v is the Bernoulli, which we introduced in Univariate Models. The NLL for the Bernoulli distribution is given by\n$$ \\text{NLL}(\\theta)=-log \\prod_{n=1}^{N} p(y_n|\\theta) $$ $$ = -log \\prod_{n=1}^N \\theta^{\\mathbb{I}(y_n=1)}(1-\\theta)^{\\mathbb{I}(y_n=1)} $$ $$ =-\\sum_{n=1}^{N}[\\mathbb{I}(y_n=1)\\log(\\theta)+\\mathbb{I}(y_n=0)\\log(1-\\theta)] $$ $$ = -[N_1\\log(\\theta)+N_0\\log(1-\\theta)] $$\nwhere we have defined $N_1 = \\sum_{n=1}^{N} \\mathbb{I}(y_n = 1)$ and $N_0 = \\sum_{n=1}^{N} \\mathbb{I}(y_n = 0)$, representing the number of heads and tails. (The NLL for the binomial is the same as for the Bernoulli, modulo an irrelevant $\\binom{N}{c}$ term, which is a constant independent of $\\theta$.) These two numbers are called the sufficient statistics of the data, since they summarize everything we need to know about $\\mathcal{D}$. The total count, $N = N_0 + N_1$, is called the sample size. The MLE can be found by solving $\\frac{d}{d\\theta} \\text{NLL}(\\theta) = 0$. The derivative of the NLL is\n$$ \\frac{d}{d\\theta} \\text{NLL}(\\theta) = \\frac{-N_1}{\\theta} + \\frac{N_0}{1 - \\theta} $$\nand hence the MLE is given by\n$$ \\widehat\\theta_{\\text{mle}} = \\frac{N_1}{N_0 + N_1} $$\nWe see that this is just the empirical fraction of heads, which is an intuitive result.\nExample: MLE for the categorical distribution Suppose we roll a K-sided dice N times. Let $Y_n \\in {1,\u0026hellip;,K}$ be the n\u0026rsquo;th outcome, where $Y_n \\sim Cat(\\theta)$. We want to estimate the probabilities $\\theta$ from the dataset $\\mathcal{D}= (y_n :n=1:N)$. The NLL is given by\n$$ \\text{NLL}(\\theta)=-\\sum_k N_k log(\\theta_k) $$\nwhere $N_k$ is the number of times the event $Y=k$ is observed. (The NLL for the multinomial is the same, up to irrelevant scale factors.) To compute the MLE, we have to minimize the NLL subject to the constraint that $\\sum_{k=1}^{K} \\theta_k =1$. To do this, we will use the method Lagrange multiplies (Optimization).\n","permalink":"http://localhost:1313/posts/statistic/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn the section \u003ca href=\"https://nguyentuss.github.io/p/univariate-models/\"\u003eUnivariate Models\u003c/a\u003e and  \u003ca href=\"https://nguyentuss.github.io/p/multivariate-models/\"\u003eMultivariate Models\u003c/a\u003e, we assumed all the parameters $\\theta$ is known. In this section, we discuss how to learn these parameters from data.\nThe process of estimating $\\theta$ from $\\mathcal{D}$ is call \u003cstrong\u003emodel fitting\u003c/strong\u003e, or \u003cstrong\u003etraining\u003c/strong\u003e, and is at the heart of machine learning. There are many methods for producing such estimates, but most boil down to an optimization problem of the form.\n$$\n\\widehat{\\theta} = \\arg\\min_{\\theta} \\mathcal{L}(\\theta)\n$$\nwhere $\\mathcal{L(\\theta)}$ is some kind of loss function or objective function. We discuss several different loss functions in this chapter. In some cases we also discuss how to solve the optimization problem in closed form. In general, however we will need to use some kind of generic optimization algorithm, which we will discuss in \u003ca href=\"https://nguyentuss.github.io/p/optimization/\"\u003eOptimization\u003c/a\u003e.\nIn addition to computing a \u003cstrong\u003epoint estimate $\\widehat{\\theta}$\u003c/strong\u003e. We discuss how to model our uncertainty or confidence in this estimate. In statistics, the process of quantifying uncertainty about an unknown quantity estimated from a finite sample of data is called \u003cstrong\u003einference\u003c/strong\u003e.\u003c/p\u003e","title":"Statistic"},{"content":"KL Divergence Kullback-Leibler (KL) divergence measures how one probability distribution diverges from a second, expected distribution.\nDefinition For discrete distributions $P$ and $Q$ over a set $\\mathcal{X}$:\n$$ D_{KL}(P \\parallel Q) = \\sum_{x \\in \\mathcal{X}} P(x) \\log \\frac{P(x)}{Q(x)} $$\nFor continuous distributions with densities $p(x)$ and $q(x)$:\n$$ D_{KL}(P \\parallel Q) = \\int_{-\\infty}^{\\infty} p(x) \\log \\frac{p(x)}{q(x)} , dx $$\nIntuition KL divergence quantifies the extra information needed when using $Q$ instead of the true distribution $P$. It is asymmetric, so generally: $$ D_{KL}(P \\parallel Q) \\ne D_{KL}(Q \\parallel P) $$\nProperties Non-negativity: $$ D_{KL}(P \\parallel Q) \\ge 0 $$ Equality holds if $P = Q$.\nNot a true distance metric (not symmetric and no triangle inequality).\nExample Let:\n$$ P(x) = \\begin{cases} 0.4 \u0026amp; x_1 \\ 0.3 \u0026amp; x_2 \\ 0.3 \u0026amp; x_3 \\end{cases} \\quad Q(x) = \\begin{cases} 0.5 \u0026amp; x_1 \\ 0.2 \u0026amp; x_2 \\ 0.3 \u0026amp; x_3 \\end{cases} $$\nThen:\n$$ D_{KL}(P \\parallel Q) = 0.4 \\log(0.4/0.5) + 0.3 \\log(0.3/0.2) + 0.3 \\log(0.3/0.3) $$\nEntropy Entropy is a measure of the uncertainty or information content of a random variable.\nDefinition For a discrete random variable $X$ with distribution $P(x)$:\n$$ H(X) = -\\sum_{x \\in \\mathcal{X}} P(x) \\log P(x) $$\nIf $\\log$ is base 2 → Entropy is in bits.\nIf $\\log$ is base $e$ → Entropy is in nats.\nIntuition Higher entropy = more surprise or uncertainty. Lower entropy = more predictable outcomes. Properties Non-negativity: $$ H(X) \\ge 0 $$\nMaximum entropy when all outcomes are equally likely:\n$$ H_{\\text{max}} = \\log n $$\nwhere $n$ is the number of outcomes.\nAdditivity for independent variables:\n$$ H(X, Y) = H(X) + H(Y) $$\nExample Fair coin: $$ P(H) = 0.5,\\quad P(T) = 0.5 $$\n$$ H(X) = -0.5 \\log_2 0.5 - 0.5 \\log_2 0.5 = 1 \\text{ bit} $$\nBiased coin: $$ P(H) = 0.9,\\quad P(T) = 0.1 $$\n$$ H(X) = -0.9 \\log_2 0.9 - 0.1 \\log_2 0.1 \\approx 0.47 \\text{ bits} $$\nApplications Information Theory: Data compression, transmission. Machine Learning: Decision trees, classification loss functions. Physics: Thermodynamic disorder. Cryptography: Measuring randomness and uncertainty. Connection to KL Divergence KL divergence can also be expressed in terms of entropy:\n$$ D_{KL}(P \\parallel Q) = -H(P) - \\sum_x P(x) \\log Q(x) $$\n","permalink":"http://localhost:1313/posts/information-theory/","summary":"\u003ch2 id=\"kl-divergence\"\u003eKL Divergence\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eKullback-Leibler (KL) divergence\u003c/strong\u003e measures how one probability distribution diverges from a second, expected distribution.\u003c/p\u003e\n\u003ch3 id=\"definition\"\u003eDefinition\u003c/h3\u003e\n\u003cp\u003eFor discrete distributions $P$ and $Q$ over a set $\\mathcal{X}$:\u003c/p\u003e\n\u003cp\u003e$$\nD_{KL}(P \\parallel Q) = \\sum_{x \\in \\mathcal{X}} P(x) \\log \\frac{P(x)}{Q(x)}\n$$\u003c/p\u003e\n\u003cp\u003eFor continuous distributions with densities $p(x)$ and $q(x)$:\u003c/p\u003e\n\u003cp\u003e$$\nD_{KL}(P \\parallel Q) = \\int_{-\\infty}^{\\infty} p(x) \\log \\frac{p(x)}{q(x)} , dx\n$$\u003c/p\u003e\n\u003ch3 id=\"intuition\"\u003eIntuition\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eKL divergence quantifies the \u003cstrong\u003eextra information\u003c/strong\u003e needed when using $Q$ instead of the true distribution $P$.\u003c/li\u003e\n\u003cli\u003eIt is \u003cstrong\u003easymmetric\u003c/strong\u003e, so generally:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e$$\nD_{KL}(P \\parallel Q) \\ne D_{KL}(Q \\parallel P)\n$$\u003c/p\u003e","title":"Information Theory"},{"content":"Truncation Errors and the Taylor Series Truncation errors are those that result from using approximation in place of an exact mathematical procedure. $$ \\frac{dv}{dt} \\approx \\frac{\\Delta v}{\\Delta t} = \\frac{v(t_{i+1})-v(t_i)}{t_{i+1}-t_i}$$ A truncation error was introduced into the numerical solution because the difference equation only approximates the true value of the derivative. In order to gain insight into the properties of such errors, we now turn to a mathematical formulation that is used widely in numerical methods to express functions in an approximate fashion— the Taylor series.\nThe Taylor Series Taylor’s theorem (Box 4.1) and its associated formula, the Taylor series, is of great value in the study of numerical methods. In essence, the Taylor series provides a means to predict a function value at one point in terms of the function value and its derivative at another point. In particular, the theorem states that any smooth function can be approximated as a polynomial. A useful way to gain insights into Taylor series is to build it term by term. For example, the first term in the series is $$ f(x_{i+1} \\approx f(x_i)) $$ This relationship, called the zero-order approximation, indicates that the value of $f$ at the new point is the same as its value at the old point. This result make intuitive sense because if $x_i$ and $x_{i+1}$ are close to each other, it is likely that the new value is probably similar to the old value.\nTaylor\u0026rsquo;s theorem If the function $f$ and its $n+1$ derivatives are continuous on an interval containing $a$ and $x$, then the value of the function at $x$ is given by. $$ f(x) = f(a) +f\u0026rsquo;(a)(x-a)+\\frac{f\u0026rsquo;\u0026rsquo;(a)}{2!}(x-a)^2+\u0026hellip;+\\frac{f^{(n)}(a)}{n!}(x-a)^n + R_n $$ where the remainder (truncation error) $R_n$ is defined as $$ R_n = \\int_{a}^{x} \\frac{(x-t)^{(n+1)}}{(n+1)!}f^{n+1} $$\nnth order approximation $$ \\begin{align} f(x_{i+1}) = f(x_i) +f\u0026rsquo;(x_i)(x_{i+1}-x_i)+\\frac{f\u0026rsquo;\u0026rsquo;}{2!}(x_{i+1}-x_i)^2+\u0026hellip;\\ +\\frac{f^{(n)}}{n!}(x_{i+1}-x_i)^n + R_n \\end{align} $$ where $(x_{i+1}-x_i)=h$ (step size)\nExample: Taylor Series Approximation of a Polynomial Problem Statement Use zero- through fourth-order Taylor series expansions to approximate the function $$ f(x)= -0.1x^4-0.15x^3-0.5x^2-0.25x+1.2$$ from $x_i=0$ with $h=1$. That is, the predict function\u0026rsquo;s value at $x_{i+1}=1$. Solution: Because we are dealing with a known function, we can compute values for $f(x)$ between $0$ and $1$. The results in the figure indicate that the function starts at $f(0)=1.2$ and the curves downward to $f(1)=0.2$. Thus the true value we w to predict is 0.2.\nThe Taylor series approximation with $n=0$ is $$ f(x_{i+1}) \\approx 1.2 $$\nThus, as in the zero-order approximation is a constant. Using this formulation results in a truncation error of\n$$ E_t = 0.2 - 1.2 = -1.0 $$\nat $x = 1$.\nFor $n = 1$, the first derivative must be determined and evaluated at $x = 0$:\n$$ f\u0026rsquo;(0) = -0.4(0.0)^3 - 0.45(0.0)^2 - 1.0(0.0) - 0.25 = -0.25 $$\nTherefore, the first-order approximation is\n$$ f(x_{i+1}) \\approx 1.2 - 0.25h $$\nwhich can be used to compute $f(1) = 0.95$. Consequently, the approximation begins to capture the downward trajectory of the function in the form of a sloping straight line. This results in a reduction of the truncation error to\n$$ E_t = 0.2 - 0.95 = -0.75 $$\nFor $n = 2$, the second derivative is evaluated at $x = 0$:\n$$ f\u0026rsquo;\u0026rsquo;(0) = -1.2(0.0)^2 - 0.9(0.0) - 1.0 = -1.0 $$\nTherefore, according to,\n$$ f(x_{i+1}) \\approx 1.2 - 0.25h - 0.5h^2 $$\nand substituting $h = 1$, $f(1) = 0.45$. The inclusion of the second derivative now adds some downward curvature resulting in an improved estimate, as seen in. The truncation error is reduced further to\n$$ 0.2 - 0.45 = -0.25. $$\nMaclaurin Series is the special case of Taylor series where $a=0$\n","permalink":"http://localhost:1313/posts/numerical-method/","summary":"\u003ch2 id=\"truncation-errors-and-the-taylor-series\"\u003eTruncation Errors and the Taylor Series\u003c/h2\u003e\n\u003cp\u003eTruncation errors are those that result from using approximation in place of an exact mathematical procedure.\n$$ \\frac{dv}{dt} \\approx \\frac{\\Delta v}{\\Delta t} = \\frac{v(t_{i+1})-v(t_i)}{t_{i+1}-t_i}$$\nA truncation error was introduced into the numerical solution because the difference equation only approximates the true value of the derivative. In order to gain insight into the properties of such errors, we now turn to a mathematical formulation that is used widely in numerical methods to express functions in an approximate fashion— the Taylor series.\u003c/p\u003e","title":"Numerical Method"},{"content":"Optical flow quantifies the motion of objects between consecutive frames captured by a camera. These algorithms attempt to capture the apparent motion of brightness patterns in the image. It is an important subfield of computer vision, enabling machines to understand scene dynamics and movement. What is brightness Lets break down the definition of brightness. In an image, brightness refers to the intensity of light at each pixel. It determines how light or dark a pixel appears.\nHow Brightness Is Represented in Images Grayscale Images Each pixel has a single intensity value ranging from: 0 (black) 255 (white) Color Images (RGB Format) Each pixel is made of Red, Green, and Blue (RGB) channels. Each channel has values from 0 to 255. Brightness is the combined intensity of all three channels. How to Measure Brightness? For grayscale images: Brightness is just the pixel value. For color images: A common way to measure brightness is: $$ brightness = \\frac{R+B+G}{3} $$ Basic Gradient-Based Estimation A common starting point for optical flow estimation is to assume that pixel intensities are translated from one frame to next $$ I(\\vec{x},t)=I(\\vec{x}+\\vec{u},t+1) \\tag{1}$$ where $I(\\vec{x},t)$ is an image intensity as a function of space $\\vec{x}=(x,y)^T$ and time $t$, and $\\vec{u}=(u_1,u_2)^T$ is the 2D velocity. That equation only holds under brightness constancy assumption. Of course, brightness constancy rarely hold exactly. The underlying assumptions that surface radiance remains fixed from one frame to next. One can fabricate scenes for which this holds, the scene might be constrained to contain only Lambertian Surface (no secularities), with a distant point source (so that changing the distance to the light source has no effect), no object rotations, and no secondary illumination (shadows or inter-surface reflection). Although this is unrealistic, it is remarkable that the brightness constancy assumption works well in practice. To derive an estimator for 2D velocity $\\vec{u}$, we first consider the 1D case. Let $f_1(x)$ and $f_2(x)$ be 1D signals (images) at two time instants. In the figure above, suppose that $f_2(x)$ is a translated version $f_1(x)$, let $f_2(x)=f_1(x-d)$ where d denotes the translation. A Taylor series expansion of $f_1(x-d)$ about x is given by\n$$ f_1(x-d) = f_1(x) -df\u0026rsquo;_1(x) + \\frac{d^2}{2!}f\u0026rsquo;\u0026rsquo;1(x)-\u0026hellip;= \\sum{0}^{\\infty}\\frac{(-d)^n}{n!}f^{(n)} \\tag{2}(x) $$\nWith this expansion, we can rewrite the difference between two signals at location $x$\n$$ f_1(x)-f_2(x) = df_1(x)-O(d^2f\u0026rsquo;\u0026rsquo;_1) \\tag{3} $$\nIgnoring the second- and higher-order terms, we obtain the approximation at $x$\n$$ \\hat{d}=\\frac{f_1(x)-f_2(x)}{f\u0026rsquo;_1(x)} \\tag{4} $$\nThe 1D case generalizes straightforwardly to 2D. Assume that the displaced image is well approximated y the first-order Taylor series:\n$$ I(\\vec{x}+\\vec{u},t+1)\\approx I(\\vec{x},t)+\\vec{u}\\nabla I(\\vec{x},t)+I_t(\\vec{x},t)\\tag{5}$$\nwhere $\\nabla I(\\vec{x},t)=(I_x,I_y)$, which $I_x,I_y$ show how brightness changes in the x and y directions and $I_t$ show how brightness changes over time of image $I(\\vec{x},t)$ at time $t$. If $I_t\u0026gt;0$, the pixel is getting brighter and $I_t\u0026lt;0$ otherwise, and $\\vec{u}=(u_1,u_2)^T$ is the 2D velocity. Ignoring the high-order Taylor series, substitute with equation $(1)$, we obtain\n$$\\nabla I(\\vec{x},t)\\vec{u}+I_t(\\vec{x},t)=0 \\tag{6}$$ This equation relates the velocity to the space-time image derivatives at one image location, and is often called the gradient constraint equation or Optical flow constraint equation(OFCE). If one has access to only two frames, or cannot estimate $I_t$, it is straight-forward to derive a closely related gradient constraint, in which $I_t(\\vec{x},t)$ is replaced by $\\delta I(\\vec{x},t) \\triangleq I(\\vec{x},t+1)-I(\\vec{x},t)$\nIntensity Conversation Imagine you are watching a video where a white ball moves across a dark background. As the ball moves, its brightness (intensity) remains unchanged, meaning the white color of the ball does not fade or darken while it moves. Let\n$I(x,y,t)$ represent the brightness (grayscale intensity) at position $(x,y)$ at time $t$. The ball moves along a path $\\vec{x}=(x(t),y(t))$. The intensity of the ball (its brightness) remains constant at $c$. From the equation $$ I(\\vec{x}(t),t)=c \\tag{7} $$ This means that any time t, the intensity in the track remain the same, regardless of its positions. The temporal derivative of which yields $$ \\frac{d}{dt}I(\\vec{x}(t),t)=0 \\tag{8} $$ Expanding the left-hand-side, using chain rule give us $$ \\frac{d}{dt}I(\\vec{x}(t),t)=\\frac{\\partial I}{\\partial x}\\frac{\\partial x}{\\partial t} + \\frac{\\partial I}{\\partial y}\\frac{\\partial y}{\\partial t} + \\frac{\\partial I}{\\partial t}\\frac{\\partial t}{\\partial t} = \\nabla I(\\vec{x},t)\\vec{u} + I_t \\tag{9} $$ where the path derivative it just the optical flow $\\vec{u} \\triangleq (dx/dt,dy/dt)^T$. Combine $(8),(9)$ we have gradient constraint equation. Least-Squares Estimation Once cannot recover u from one gradient constraint since $(6)$ is one equation with two unknown, $u_1$ and $u_2$. The intensity gradient constrains the flow to a one parameter family of velocities along a line in velocity space. Two vectors are perpendicular if their dot product is zero:\n$$ \\nabla I \\cdot \\vec{d} = 0 $$\nwhere:\n$\\nabla I = (I_x, I_y)$ (gradient direction), and $\\vec{d}$ is the direction vector of the constraint line. From the equation $I_x , u + I_y , v = -I_t$ we can rewrite the direction vector of the line as: $$ \\vec{d} = (I_y, -I_x) $$ Now, compute the dot product:\n$$ (I_x, I_y) \\cdot (I_y, -I_x) = I_x I_y + I_y (-I_x) = I_x I_y - I_x I_y = 0 $$ So that the optical flow constraint line is perpendicular to $\\nabla I$ and its perpendicular distance from the origins $|I_t|/||\\nabla I||$. One common way to further constrain $\\vec{u}$ is to use the gradient constraints, assuming they share the same 2D velocity. With many constraints there may be no velocity that simultaneously satisfies them all, so instead we find the velocity that minimizes the constraint errors. The least-squares (LS) estimator minimizes the squared errors $$ E(\\mathbf{u}) ;=; \\sum_{\\vec{x}} g(\\vec{x}) \\Bigl[;\\vec{u} ,\\cdot, \\nabla I(\\vec{x}, t) ;+; I_t(\\vec{x}, t)\\Bigr]^{2} $$\nwhere $g(\\vec{x})$ is a weighting function that determines the support of the estimator (the region within which we combine constraints). It is common to let $g(\\vec{x})$ be Gaussian in order to weight constraints in the center of the neighborhood more highly, giving them more influence. The 2D velocity $\\hat{u}$ that minimizes $E(\\vec{u})$ is the least squares flow estimate. The minimum of $E(\\vec{u})$ can be found from its critical points, where its derivatives with respect to $\\vec{u}$ are zero; i.e.,\n$$ \\frac{\\partial E(u_1, u_2)}{\\partial u_1}=\\sum_{\\vec{x}} g(\\tilde{x})\\Bigl[u_1 ,I_x^2+ u_2 , I_x , I_y+ I_t , I_x\\Bigr]= 0 $$\n$$ \\frac{\\partial E(u_1, u_2)}{\\partial u_2} =\\sum_{\\vec{x}} g(\\vec{x})\\Bigl[u_1 , I_xI_y + u_2 , I_y^2+ I_t , I_y\\Bigr]= 0 $$ These equations can we rewritten in matrix form\n$$ \\mathbf{M}\\vec{u}=\\vec b $$ Where $\\mathbf{M}$ and $\\vec{b}$ are $$ \\mathbf{M} = \\begin{bmatrix} \\sum g I_x^2 \u0026amp; \\sum g I_xI_y \\ \\sum gI_y^2 \u0026amp; \\sum g I_x I_y \\end{bmatrix} , \\vec{b} = - \\begin{pmatrix} I_tI_x \\ I_tI_y \\end{pmatrix} $$ When $\\mathbf{M}$ has rank 2, then the LS estimate is $\\hat{u}=\\mathbf{M}^{-1}\\vec{b}$\nFind root Back to the equation OCFE, this gives you one equation but find two unknowns $(u,v)$, this means the solution cannot be determined uniquely with a single constraint (a single pixel), We need more information to determine the correct motion for each pixel. So there are different methods handle this problem in different ways\nLucas-Kanade Optical Flow (Local Estimation) Finds $(u,v)$ in small patches (instead of a single pixel). Assumes motion is constant within a small neighborhood. Uses least-squares optimization to find $(u,v)$ for the whole patch. Horn-Schunck Optical Flow (Global Estimation) Finds $(u,v)$ for every pixel across the entire image. Uses a smoothness constraint so that neighboring pixels have similar motion. Solves $(u,v)$ as a global optimization problem. Click this link to see more informationand this Brox Optical Flow \u0026amp; Deep Learning-Based Methods Use more complex constraints (e.g., gradient constancy, deep learning models). Improve accuracy for large and complex motions Practice Reference Video Analysis Algorithms in Computer Vision Thuật toán phân tích video trong thị giác máy tính – VinBigdata Product Optical Flow Estimation | Papers With Code (recommend) 14.3 OF - HornSchunck ","permalink":"http://localhost:1313/posts/optical-flow/","summary":"\u003cp\u003eOptical flow quantifies the motion of objects between consecutive frames captured by a camera. These algorithms attempt to capture the apparent motion of brightness patterns in the image. It is an important subfield of computer vision, enabling machines to understand scene dynamics and movement.\n\u003cimg loading=\"lazy\" src=\"/posts/optical-flow/img/pic1.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"what-is-brightness\"\u003eWhat is brightness\u003c/h2\u003e\n\u003cp\u003eLets break down the definition of brightness. In an image, \u003cstrong\u003ebrightness\u003c/strong\u003e refers to the intensity of light at each pixel. It determines how \u003cstrong\u003elight or dark\u003c/strong\u003e a pixel appears.\u003c/p\u003e","title":"Optical Flow"},{"content":"Convolutional Neural Networks are very similar to ordinary Neural Networks, they are made up of neurons that have learnable weights and biases. Each neuron receives some inputs, performs a dot product and optionally follows it with a non-linearity. The whole network still expresses a single differentiable score function: from the raw image pixels on one end to class scores at the other. And they still have a loss function (e.g. SVM/Softmax) on the last (fully-connected) layer and all the tips/tricks we developed for learning regular Neural Networks still apply.\nNote: You may have knowledge about ANN and Backpropagation to read this. Some suggested reading links http://cs231n.github.io/optimization-2 and https://cs231n.stanford.edu/handouts/linear-backprop.pdf\nConvolution Definition Convolution is a concept in digital signal processing that transforms the input information through the convolution operation with a filter to yield an output in a form of a new signal. This signal will reduces the features that the filter is not concerned and just keep the main features. Each filter have their main purpose. There are many convolution n-dimension, I will talk about the 2D convolution because it is the easiest to visualize and also the most common convolution.\nThe two-dimensional convolution applied to the 2D input matrix and the 2D filter matrix. The convolution of an input matrix $X \\in \\mathbb{R}^{W\\times H \\times d}$ with a filter $F \\in \\mathbb{R}^{w\\times h \\times d}$ produces an output matrix $Y \\in \\mathbb{R}^{W\\times H}$. The steps are follows:\nCompute the convolution at the single point: Position the filter at the top-left corner of the input matrix, resulting in a submatrix $X_{sub}$ whose size matches the filter\u0026rsquo;s dimension. The first value, $y_{11}$ is the convolution of $X_{sub}$ with $F$, such as $$ y_{11} = \\sum_{}^{w}\\sum_{}^{h}\\sum_{}^{d} x_{sub}[i,j,u] \\times f[i,j,u] $$ Slide the window: Next, slide the filter window across the input matrix from left to right, and then from top to bottom, using the specified stride. For each position, compute the corresponding output value. Once you have traversed the entire input, you obtain the complete output matrix $Y$. (Click this link to futher more information about this technique) In a convolutional neural network (CNN), each subsequent layer takes the output from the layer immediately before it. Therefore, to keep the network design manageable, we need to determine the output size for each layer. This means that given the input (matrix) size $(W_1,H_1)$, a filter of size $(F,F)$, and a stride $S$, we can determine the output matrix $(W_2,H_2)$. Consider the process sliding with size $1\\times W_1$\nAssume the process will stop at $W_2$ step. At the first step will reach to position $F$. After each step we will move about $S$, so step $i$ will reach to position $F + (i-1)S$. So that the final step $W_2$ matrix will reach to $F+(W_2-1)S$. This is the highest and closest with $W_1$. In the perfect circumstance the same position $F+(W_2-1)S=W_1$. $$ W_2=\\frac{W_1-F}{S}+1 $$ If there are not in that condition, the division just take the integer, this equation will be $$ W_2=\\lfloor \\frac{W_1-F}{S}\\rfloor +1 $$ However, we can also make it in the perfect circumstance if we add zero-padding on the both edge bound with size $P$ so that the division will divisible by $S$\nThe equation will be $$ W_2=\\frac{W_1-F+2P}{S}+1 $$ similarly with $H$ $$ H_2=\\frac{H_1-F+2P}{S}+1 $$\nPractice We need to set up such as import lib and load images\nimport numpy as np import os import cv2 from Convolution import conv2d import matplotlib.pyplot as plt data_path = \u0026#34;data/\u0026#34; if not os.path.exists(data_path): print(f\u0026#34;Error: Path does not exist - {os.path.abspath(data_path)}\u0026#34;) data_list = os.listdir(data_path) %load_ext autoreload %autoreload 2 Compute the convolution in 2D\ndef conv2d(X, F, s=1, p=0): \u0026#34;\u0026#34;\u0026#34; X: matrix input F: filter s: step jump p: padding \u0026#34;\u0026#34;\u0026#34; (W, H) = X.shape f = F.shape[0] # Output dimensions w = (W - f + 2 * p) // s + 1 h = (H - f + 2 * p) // s + 1 X_pad = np.pad(X, pad_width=((p,p),(p,p)), mode=\u0026#39;constant\u0026#39;, constant_values=0) # print(w,h) Y = np.zeros((w, h)) for i in range(w): for j in range(h): x = i * s y = j * s Y[i][j] = np.sum(X_pad[x:(x+f),y:(y+f)]*F) return Y In this example, I will use emboss filter and calculate the convolution on each image\n# emboss filter filter = np.array([[-2,-1,0] ,[-1,1,1] ,[0,1,2]]) for file in data_list: img_path = os.path.join(data_path,file) img = cv2.imread(img_path) #Convert to grey img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) plt.figure(figsize=(8,6)) plt.subplot(1,2,1) plt.imshow(img) #Change to np array img = np.array(img) blur_img = conv2d(img, filter, s = 1, p=5) plt.subplot(1,2,2) plt.imshow(blur_img) plt.show() Convolution Neural Network Definition In machine learning, a classifier assigns a class label to a data point. For example, an image classifier produces a class label (e.g, bird, plane) for what objects exist within an image. A convolutional neural network, or CNN for short, is a type of classifier, which excels at solving this problem! A CNN is a neural network: an algorithm used to recognize patterns in data. Neural Networks in general are composed of a collection of neurons that are organized in layers, each with their own learnable weights and biases. Let’s break down a CNN into its basic building blocks.\nA tensor can be thought of as an n-dimensional matrix. In CNN above, tensors will be 3-dimensional with the exception of the output layer. A neuron can be thought of as a function that takes in multiple inputs and yields as a single output. The outputs of neurons are represented above as the activation map. A layer are the collection of the neurons in the same operations Kernel and weights and bias, while unique to each neuron, are tuned during the training phase, and allow the classifier to adapt to the problem and dataset provided. A CNN conveys a differentiable score function, which is represented as a class score in the visualization on the output layers. Before we start, I will talk some terminology in this blog and you can also see in every lessons or blogs\nUnit: A unit represents the value of a specific point in the tensor at each layer of a CNN (Convolutional Neural Network). Receptive Field: This is a region in the input image that a filter (kernel) processes during convolution. It determines which portion of the input image contributes to a particular unit in the next layer. Local Region: In a broader sense, it can include the receptive field. It refers to a specific region in the tensor at different layers of a CNN. Feature Map: This is the output matrix obtained after applying convolution operations between a filter and the receptive fields by sliding from left to right and top to bottom. Activation Map: The output of the feature map after applying an activation function to introduce non-linearity. Architecture view Regular Neural Nets: Neural Networks receive an input (a single vector), and transform it through a series hidden layer. Each hidden layer is made up of a set of neurons, where each neuron is fully connected to all neurons in the previous layer, and where neurons in a single layer function completely independently and do not share any connections. The last fully-connected layer is called the “output layer” and in classification settings it represents the class scores.\nRegular Neural Nets don’t scale well to full images. In CIFAR-10, images are only of size $32\\times 32\\times 3$ (32 wide, 32 high, 3 color channels), so a single fully-connected neuron in a first hidden layer of a regular Neural Network would have $32\\times 32\\times 3 =3072$ weights. This amount still seems manageable, but clearly this fully-connected structure does not scale to larger images. For example, an image of more respectable size, e.g. $200 \\times 200 \\times 3$, would lead to neurons that have $200\\times 200\\times 3 = 120000$ weights. Moreover, we would almost certainly want to have several such neurons, so the parameters would add up quickly! Clearly, this full connectivity is wasteful and the huge number of parameters would quickly lead to overfitting. 3D volumes of neurons. Convolutional Neural Networks take advantage of the fact that the input consists of images and they constrain the architecture in a more sensible way. In particular, unlike a regular Neural Network, the layers of a ConvNet have neurons arranged in 3 dimensions: width, height, depth. (Note that the word depth here refers to the third dimension of an activation volume, not to the depth of a full Neural Network, which can refer to the total number of layers in a network). For example, the input images in CIFAR-10 are an input volume of activations, and the volume has dimensions $32\\times 32\\times 3$ (width, height, depth respectively). As we will soon see, the neurons in a layer will only be connected to a small region of the layer before it, instead of all of the neurons in a fully-connected manner. Moreover, the final output layer would for CIFAR-10 have dimensions $1\\times 1\\times 10$, because by the end of the ConvNet architecture we will reduce the full image into a single vector of class scores, arranged along the depth dimension. In the basic CNN, depth,stride and zero-padding are the hyperparameters control the output volume. Layers used to build ConvNets A simple ConvNet is a sequence of layers, and every layers of a ConvNet transforms one volume of activations to another through a differentiable function. We use three main types of layers to build ConvNet architectures: Convolutional Layers,Pooling Layer and Fully-Connected Layers (same as the ANN). We will go into more deeply, but a simple ConvNet for CIFAR-10 classification could have the architecture, INPUT $\\rightarrow$ CONV $\\rightarrow$ RELU $\\rightarrow$ POOL $\\rightarrow$ FC\nINPUT 32x32x3 will hold the raw pixel values of the image, in this case an image of width 32, height 32, and three color RGB CONV layer will compute the output of neurons that are connected to local regions in the input, each computing a dot product between their weights and a small region they are connected to in the input volume. This may result in volume such as 32x32x12 if we decided to use 12 filters. RELU layer will apply an elementwise activation function, such as the $max(0,x)$ thresholding at zero. This leaves the size of the volume unchanged (32x32x12). POOL layer will perform a downsampling operation along the spatial dimensions (width, height), resulting in volume such as 16x16x12. FC (i.e. fully-connected) layer will compute the class scores, resulting in volume of size 1x1x10, where each of the 10 numbers correspond to a class score, such as among the 10 categories of CIFAR-10. As with ordinary Neural Networks and as the name implies, each neuron in this layer will be connected to all the numbers in the previous volume. In this way, ConvNets transform the original image layer by layer from the original pixel values to the final class scores. Note that some layers contain parameters and other don’t. In particular, the CONV/FC layers perform transformations that are a function of not only the activations in the input volume, but also of the parameters (the weights and biases of the neurons). On the other hand, the RELU/POOL layers will implement a fixed function. The parameters in the CONV/FC layers will be trained with gradient descent so that the class scores that the ConvNet computes are consistent with the labels in the training set for each image. Convolutional Layer The Conv layer is the core building block of a Convolutional Network that does most of the computational heavy lifting. Let’s first discuss what the CONV layer computes without brain/neuron analogies. The CONV layer\u0026rsquo;s parameters consist of a set of learnable filters. Every filters is a small spatially(along width and height) but extends through the full depthof the input volume.\nFor example, lets look at the first convolutional layer have $3\\times3\\times3$ (3 pixels width and height, and 3 color channels), also have padding. During the forward pass, we slide (convolve) each filter across the width and height of the input volume and compute the dot products between entries of filter and the input at any position. As we slide the filter over the width and height of the input volume, we will produce 2-dimensional activation map that gives the responses of that filter at every spatial position. Intuitively, the network will learn filters that activate when they see some type of visual features such as an edge of some orientation or a blotch (patch) of some color on the first layer, or eventually entire honeycomb on higher layers of the network. Now we will have an entire set of filters in each CONV layer (e.g. 12 filters), and each of them will produce a separated 2-dimensional activation map. We will stack these activation maps along the depth dimension and produce the output volume. Furthermore, every entry in 3D output volume can also be interpreted an output of a neuron that look at only a small region in the input and shares the parameters will all neurons to the left and right.\nLocal connectivity When dealing with high-dimensional inputs such as images, as we saw above it is impractical to connect neurons to all neurons in the previous volume. Instead, we will connect each neuron to only a local region of the input volume. The spatial extent of this connectivity is a hyperparameter called the receptive field of the neuron (equivalently this is the filter size). The extent of the connectivity along the depth axis is always equal to the depth of the input volume. It is important to emphasize again this asymmetry in how we treat the spatial dimensions (width and height) and the depth dimension: The connections are local in 2D space (along width and height), but always full along the entire depth of the input volume. For example, suppose that the input volume has size $32\\times 32\\times 3$, (e.g. an RGB CIFAR-10 image). If the receptive field (or the filter size) is $5\\times 5$, then each neuron in the Conv Layer will have weights to a $5 \\times 5\\times 3$ region in the input volume, for a total of $5\\times 5\\times 75$ weights (and +1 bias parameter). Notice that the extent of the connectivity along the depth axis must be 3, since this is the depth of the input volume.\nZero-Padding In the figure above on the left, the input dimension was 5 and the output dimension was equal, 5. This worked out because our receptive field were 3 and we used zero-padding of 1. If there are no zero-padding used, then the output volume would have spatial dimension only 3. In gereral, setting zero-padding to be $P=(F-1)/2$ when the stride $S=1$ to ensures that the input volume and the output volume have the same size spatially.\nStrides Note again that the spatial arrangement hyperparameters have mutual constraints. For example, when the input has size $W=10$, no zero-padding is used $P=0$, and the filter size $F=3$, then it possible to use stride $S=2$, since $W_2=(W-F+2P)/S=4.5$, not an integer indicating the neurons don\u0026rsquo;t fit neatly and symmetrically across the input, and a ConvNet library could thrown exception or zero pad the rest to make it fit, or crop the input to make it fit.\nCarefully with choosing As I mentioned before in the strides part. In that example, assume you want to round the $4.5$ to $4$ or $5$ to make the $W_2$ be integer, that\u0026rsquo;s not a good idea, when the rounding are applied, this will cause the output shape might not match expectations, can cause misalignment in the feature map, when rounded down, some neurons can be missing, when round up, extra neurons will assumed.\nReal-world example The Krizhevsky et al. architecture that won the ImageNet challenge in 2012 accepted images of size $227\\times 227\\times 3$. On the first Convolutional Layer, it used neurons with receptive field size $F=11$, stride $S=4$ and no zero padding $P=0$. Since $(227 - 11)/4 + 1 = 55$, and since the Conv layer had a depth of $K=96$, the Conv layer output volume had size $55\\times 55\\times 96$. Each of the $55\\times 55\\times 96$ neurons in this volume was connected to a region of size $11\\times 11\\times 3$ in the input volume. Moreover, all $96$ neurons in each depth column are connected to the same $11\\times 11\\times 3$ region of the input, but of course with different weights.\nParameter Sharing Parameter sharing scheme is used in Convolutional Layers to control the number of parameters. Using in the the real-world example above, we see that there are $55\\times 55 \\times 96=290400$ neurons in the first Conv Layer, and each has the $11\\times 11\\times 3=363$ weight and $1$ bias. Together, this we will have $290400\\times 363=105415200$ parameters\n1x1 Convolution A 1×1 convolution applies a dot product across the depth (channels) of the input feature map. Instead of capturing spatial relationships (like 3×3 or 5×5 convolutions), it mixes information across channels.\nLet takes an example: Applying a 1×1 Convolution to a 32×32×3 Image.\nSuppose we have an image of size 32×32×3 (height × width × depth). The depth 3 represents the three RGB channels. Let\u0026rsquo;s say we apply N = 5 different 1×1 convolutional filters. Each 1×1 filter is actually a small matrix of shape (1×1×3)—one value for each input channel. For each spatial location (x, y) in the 32×32 grid, the 1×1 filter performs a dot product with the depth (3 channels).\nMathematically, if the input pixel at position $(i, j)$ is:\n$$ (i,j) = [R, G, B] $$\nand the 1×1 filter has weights: $$ \\text{Filter} = [w_1, w_2, w_3] $$ Then the output value at that location is: $$ \\text{Output}(i,j) = R \\cdot w_1 + G \\cdot w_2 + B \\cdot w_3 $$\nThis operation is repeated for every filter, so if we have 5 filters, we get 5 new feature maps, meaning the output shape is 32×32×5.\nPooling Layer Pooling layer usually use between the convolution layers, which reduce the spatial size to reduce an amount of the parameters and computation in the network but still keep the important features, and hence control fitting.\nThe Pooling Layer operates independently on every depth slice of the input and resizes it spatially, using the MAX operation. The most common form is a pooling layer with filters of size 2x2 applied with a stride of 2 downsamples every depth slice in the input by 2 along both width and height, discarding 75% of the activations. Every MAX operation would in this case be taking a max over 4 numbers (little 2x2 region in some depth slice). The depth dimension remains unchanged.\nGeneral Pooling. In addition to max pooling, the pooling units can also perform other functions, such as average pooling or even L2-norm pooling. Average pooling was often used historically but has recently fallen out of favor compared to the max pooling operation, which has been shown to work better in practice.\nFully connected layer After doing convolutional and pooling layer, the model have learn enough features of the image, so the final step is combined it into one. This is called Fully connected layer, similarly with ANN, flatten the layer with tensor size $H\\times W \\times D$ with vector of size $H\\times W \\times D$.\nLayer Sizing Patterns The input layer (image) should be divisible by 2 many times. Common numbers include 32 (CIFAR-10), 64,96 (STL-10) or 224 (ImageNet ConvNets).\nThe conv layers should be using small filter (e.g. 3x3 or at most 5x5), using a stride $S=1$. Padding (P) with zeros is applied to maintain the spatial dimensions of the input. For example, for F = 3 (3×3 filter), use P = 1 to keep the output size the same, for F = 5 (5×5 filter), use P = 2 to maintain input size. More generally, P = (F - 1) / 2 ensures the output has the same spatial dimensions as the input.\nThe pool layers are in charge of downsampling the spatial dimensions of the input. The most common setting is to use max-pooling with 2x2 receptive field ($F=2$) and with a stride 2 ($S=2$). Note that this discards exactly 75% of the activations in an input volume. Another slightly less common setting is to use 3x3 receptive fields with a stride of 2, but this makes \u0026ldquo;fitting\u0026rdquo; more complicated. It is very uncommon to see receptive field sizes for max pooling that are larger than 3 because the pooling is too lossy and aggressive. This usually leads to worse performance.\nReducing sizing headaches. The scheme presented above is pleasing because all the CONV layers preserve the spatial size of their input, while the POOL layers alone are in charge of down-sampling the volumes spatially. In an alternative scheme where we use strides greater than 1 or don’t zero-pad the input in CONV layers, we would have to very carefully keep track of the input volumes throughout the CNN architecture and make sure that all strides and filters “work out”, and that the ConvNet architecture is nicely and symmetrically wired.\nWhy use stride of 1 in CONV? Smaller strides work better in practice. Additionally, as already mentioned stride 1 allows us to leave all spatial down-sampling to the POOL layers, with the CONV layers only transforming the input volume depth-wise.\nWhy use padding? In addition to the aforementioned benefit of keeping the spatial sizes constant after CONV, doing this actually improves performance. If the CONV layers were to not zero-pad the inputs and only perform valid convolutions, then the size of the volumes would reduce by a small amount after each CONV, and the information at the borders would be “washed away” too quickly.\nCompromising based on memory constraints. In some cases (especially early in the ConvNet architectures), the amount of memory can build up very quickly with the rules of thumb presented above. For example, filtering a 224x224x3 image with three 3x3 CONV layers with 64 filters each and padding 1 would create three activation volumes of size 224x224x64. This amounts to a total of about 10 million activations, or 72MB of memory (per image, for both activations and gradients). Since GPUs are often bottlenecked by memory, it may be necessary to compromise. In practice, people prefer to make the compromise at only the first CONV layer of the network. For example, one compromise might be to use a first CONV layer with filter sizes of 7x7 and stride of 2 (as seen in a ZF net). As another example, an AlexNet uses filter sizes of 11x11 and stride of 4.\nCase studies Reference CNN Explainer(CNN visualization) Image Kernels explained visually(Convolution visualization) CS231n Convolutional Neural Networks for Visual Recognition Khoa học dữ liệu probml.github.io/pml-book/book1.html(book) ","permalink":"http://localhost:1313/posts/cnn/","summary":"\u003cp\u003eConvolutional Neural Networks are very similar to ordinary Neural Networks, they are made up of neurons that have learnable weights and biases. Each neuron receives some inputs, performs a dot product and optionally follows it with a non-linearity. The whole network still expresses a single differentiable score function: from the raw image pixels on one end to class scores at the other. And they still have a loss function (e.g. SVM/Softmax) on the last (fully-connected) layer and all the tips/tricks we developed for learning regular Neural Networks still apply.\u003c/p\u003e","title":"Convolution Neural Network"},{"content":" There are many different methods in computer vision. When it comes to image classification, we can apply families of CNN models such as Inception Net, MobileNet, ResNet, DenseNet, AlexNet, U-Net, and so on. For object detection, models like YOLO, SSD, Faster R-CNN, Fast R-CNN, and Mask R-CNN are commonly used. All the above algorithms belong to the deep learning category. However, before the deep learning boom, what algorithms were typically used in image processing? Today, we will explore a classic yet highly effective algorithm in image processing: HOG (Histogram of Oriented Gradients). This algorithm generates features description for the purpose of object detection. From an image, two key matrices are extracted to store essential information: gradient magnitude and gradient orientation. By combining these two pieces of information into a histogram distribution—where the gradient magnitude is counted in bins according to gradient orientation—we obtain a HOG feature vector that represents the histogram. This is the basic concept, but in practice, the algorithm is more complex. The HOG vector is computed over local regions, similar to how CNNs operate, followed by local normalization to standardize measurements. Finally, the overall HOG vector is aggregated from all local vectors.\nHOG Application There are some applications using HOG and which have high accuracy.\nHuman detection: This application first represents in Histogram of Orient Gradients for Human Detection). HOG can detect one or more people walking on a street in an image. Face detection: HOG is a good algorithm on this problem. It has the ability to represent the main contours of the face based on the direction and gradient magnitude through vectors for each cell as shown bellow Recognizing Various Objects: In addition, there are many cases of object recognition in static images, such as vehicles, traffic signals, animals, or even moving images extracted from videos.\nCreating Features for Image Classification Tasks: Many image classification tasks are built on small-sized datasets, where using deep learning networks may not always be effective and can lead to overfitting. The reason is that a small amount of data is often insufficient for training a model to accurately recognize object features. In such cases, using HOG (Histogram of Oriented Gradients) for feature extraction can yield better results. Specifically, I will also demonstrate an example at the end.\nTerminology Before diving into the HOG algorithm, I will first explain the terms used:\nFeature Descriptor: A feature descriptor is a representation of an image or an image patch that simplifies the image by extracting useful information and throwing away extraneous information.\nHistogram: A histogram is a graphical representation of the distribution of color intensities across different value ranges.\nGradient: The derivative or vector of color intensity changes that helps detect movement directions of objects in an image.\nLocal cell: A local cell is a small region in an image. In the HOG algorithm, an image is divided into multiple cells based on a square grid. Each cell is called a local cell.\nLocal portion: A local region is a section of the image where feature extraction is performed. In the algorithm, this local region is referred to as a \u0026ldquo;block.\u0026rdquo;\nLocal normalization: Normalization is performed within a local region. It is usually divided by either L2 norm or L1 norm. The purpose of normalization is to standardize color intensity values, making the distribution more consistent. This will be explained in more detail in the algorithm section.\nGradient direction: The gradient direction represents the angle between the gradient vector components $x$ and $y$, which helps determine the direction of intensity change or, in other words, the direction of shading in the image. Given that $G_x$​ and $G_y$​ are the gradient values along the $x$ and $y$ axes of the image, the gradient direction is calculated as:\n$$\\theta = \\arctan\\left(\\frac{G_y}{G_x}\\right)$$\nGradient magnitude: The magnitude of the gradient represents the length of the vector combining the gradients along the $x$ and $y$ directions. The histogram representation of this vector is used to describe the HOG features. The gradient magnitude is computed as:\n$$∣G∣=\\sqrt{G_x^2+G_y^2}$$​\nDefinition The key point in the working principle of HOG is that the local shape of an object can be described using two matrices: the gradient magnitude matrix and the gradient direction matrix. First, the image is divided into a grid of square cells, where many adjacent or overlapping local regions are identified. These regions are similar to the local image regions used in convolutional operations in CNNs. The main purpose of HOG feature extraction is to capture the local shape and edge information of an image. It works by computing the gradients (i.e., the changes in intensity) and then building histograms of these gradients over small, localized regions. This process allows the algorithm to effectively describe the appearance and structure of objects in an image, such as the contours and silhouettes of people.\nA local region consists of multiple local cells (in HOG, there are 4 cells) with a size of 8×8 pixels Then, a histogram of gradient magnitudes is computed for each local cell. The HOG descriptor is formed by concatenating the four histogram vectors corresponding to each cell into a single combined vector. To improve accuracy, each value in the histogram vector of a local region is normalized using either L2-norm or L1-norm. This normalization aims to enhance invariance to lighting and shading changes. The HOG descriptor has several key advantages over other feature descriptors:\nSince it operates on local cells, it is invariant to geometric transformations and brightness changes. Furthermore, as Dalal and Triggs discovered, applying local region normalization allows the descriptor to ignore minor body movements in pedestrian detection, as long as the person maintains an upright posture. This makes HOG particularly well-suited for human detection in images. How HOG Works (Step-by-step) Preprocessing In every image processing algorithm, the first step is preprocessing image. As mentioned earlier HOG feature descriptor used for pedestrian detection is calculated on a $64×128$ patch of an image. Of course, an image may be of any size. Typically, patches at multiple scales are analyzed at many image locations. The only constraint is that the patches being analyzed have a fixed aspect ratio. In our case, the patches need to have an aspect ratio of $1:2$. For example, they can be $100×200$, $128×256$, or $1000×2000$ but not $101×205$.\nWe need to adjust the image to grayscale\nimport numpy as np import cv2 import matplotlib.pyplot as plt #Read the image img = cv2.imread(\u0026#34;./img/bolt.jpg\u0026#34;) # Load BGR format if img is None: print(\u0026#34;Image not loaded. Check the file path and file integrity.\u0026#34;) else: # Convert BGR to RGB for proper color display in matplotlib img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) #Convert to grayscale gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) print(img_rgb.shape) print(gray.shape) #Show the images plt.figure(figsize=(8,6)) plt.subplot(1,2,1) plt.imshow(img_rgb) plt.title(\u0026#34;Original\u0026#34;) plt.subplot(1,2,2) plt.imshow(gray) plt.title(\u0026#34;Gray Image\u0026#34;) plt.show() Compute the gradient To calculate a HOG descriptor, we need to first calculate the horizontal and vertical gradients, the common way to compute is using the Sobel operator\nHorizontal gradient $$G_x = \\begin{bmatrix} -1 \u0026amp; 0 \u0026amp; 2 \\ -2 \u0026amp; 0 \u0026amp; 2 \\ -1 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix}*\\textbf{I}$$ Vertical gradient $$G_y = \\begin{bmatrix} -1 \u0026amp; -2 \u0026amp; -1 \\ 0 \u0026amp; 0 \u0026amp; 0 \\ 1 \u0026amp; 2 \u0026amp; 1 \\end{bmatrix}\\textbf{I}$$ Where $$ be a convolution between a filter and an image Gradient magnitude $$ G=\\sqrt{G_x^2+G_y^2}$$ Gradient direction $$\\theta=\\arctan\\Bigl(\\frac{G_y}{G_x}\\Bigl)$$ #Calculate gradient gx,gy gx = cv2.Sobel(gray, cv2.CV_32F, dx = 0, dy = 1, ksize=3) gy = cv2.Sobel(gray, cv2.CV_32F, dx = 1, dy = 0, ksize=3) print(\u0026#39;gray shape: {}\u0026#39;.format(gray.shape)) print(\u0026#39;gx shape: {}\u0026#39;.format(gx.shape)) print(\u0026#39;gy shape: {}\u0026#39;.format(gy.shape)) # Compute magnitude and direction gradient g, theta = cv2.cartToPolar(gx, gy, angleInDegrees=True) print(\u0026#39;gradient format: {}\u0026#39;.format(g.shape)) print(\u0026#39;theta format: {}\u0026#39;.format(theta.shape)) The output\ngray shape: (480, 640) gx shape: (480, 640) gy shape: (480, 640) gradient format: (480, 640) theta format: (480, 640) Visualize the plot\nplt.figure(figsize=(20, 10)) plt.subplot(1, 4, 1) plt.title(\u0026#39;gradient of x\u0026#39;) plt.imshow(gx) plt.subplot(1, 4, 2) plt.title(\u0026#39;gradient of y\u0026#39;) plt.imshow(gy) plt.subplot(1, 4, 3) plt.title(\u0026#39;Magnitute of gradient\u0026#39;) plt.imshow(g) plt.subplot(1, 4, 4) plt.title(\u0026#39;Direction of gradient\u0026#39;) plt.imshow(theta) plt.show() Calculate Histogram of Gradients in 8 x 8 cells In this step, the image is divided into $8\\times8$ cells and a histogram of gradients is calculated for each $8\\times8$. One of the important reasons to use a feature description to describe a patch of an image is that it provided a compact representation. An $8\\times8$ patch contain $8\\times8\\times3=192$ pixels. The gradient of this patch contains $2$ values (magnitude and direction) per pixel which adds up to $8\\times8\\times2 = 128$ numbers include $64$ values of gradient magnitude and $64$ values of gradient direction. By the end of this section we will see how these $128$ numbers are represented using a 9-bin histogram which can be stored as an array of $9$ numbers. Not only is the representation more compact, calculating a histogram over a patch makes this representation more robust to noise. Individual gradients may have noise, but a histogram over $8\\times8$ patch makes the representation much less sensitive to noise.\nBut why $8\\times8$ patch ? Why not $32\\times32$ ? It is a design choice informed by the scale of features we are looking for. HOG was used for pedestrian detection initially. $8\\times8$ cells in a photo of a pedestrian scaled to $64\\times128$ are big enough to capture interesting features ( e.g. the face, the top of the head etc.). On the right, we see the raw numbers representing the gradients in the 8×8 cells with one minor difference — the angles are between $0$ and $180$ degrees instead of $0$ to $360$ degrees. These are called “unsigned” gradients because a gradient and it’s negative are represented by the same numbers. In other words, a gradient arrow and the one $180$ degrees opposite to it are considered the same. But, why not use the $0$ – $360$ degrees ?\nEmpirically it has been shown that unsigned gradients work better than signed gradients for pedestrian detection. Some implementations of HOG will allow you to specify if you want to use signed gradients.\nThe next step is to create a histogram of gradients in these $8\\times8$ cells. The histogram contains 9 bins corresponding to angles $0, 20, 40 … 160$. The following figure illustrates the process. We are looking at magnitude and direction of the gradient of the same $8\\times8$ patch as in the previous figure. A bin is selected based on the direction, and the vote (the value that goes into the bin) is selected based on the magnitude. Let’s first focus on the pixel encircled in blue. It has an angle (direction) of $80$ degrees and magnitude of $2$. So it adds $2$ to the $5^{th}$ bin. The gradient at the pixel encircled using red has an angle of $10$ degrees and magnitude of $4$. Since $10$ degrees is half way between $0$ and $20$, the vote by the pixel splits evenly into the two bins.\nThe approach when gradient direction doesn\u0026rsquo;t fall in any bin(like an example 10 degrees), we will use linear interpolation to divided the gradients to 2 continuous bins which gradient direction falls. Obviously, the gradient direction equal $x$ map to the gradient magnitude $y$, where $x \\in [x_0,x_1]$. It\u0026rsquo;s will fall to a point between $(l-1)$ bin and $l$ bin. So that we write $$ x_{l-1}=\\frac{x_1-x}{x_1-x_0}*y, x_{l}=\\frac{x-x_0}{x_1-x_0}*y $$\nTake a sum of each gradient magnitude belong in one bins from vector bins and we collect Histogram of Gradient\n16×16 Block Normalization In the previous step, we created a histogram based on the gradient of the image. Gradients of an image are sensitive to overall lighting. If you make the image darker by dividing all pixel values by 2, the gradient magnitude will change by half, and therefore the histogram values will change by half.\nIdeally, we want our descriptor to be independent of lighting variations. In other words, we would like to “normalize” the histogram so they are not affected by lighting variations.\nLet\u0026rsquo;s say we have an RGB color vector $[128, 64, 32]$. This length of this vector is $\\sqrt{128^2+64^2+32^2}\\approx 146.64$. This is call L2 norm. Dividing each element of this vector give us a normalized vector $[0.87,0.43,0.22]$\nNow consider another vector in which the elements are twice the value of the first vector $2 \\times [ 128, 64, 32 ] = [ 256, 128, 64 ]$. You can work it out yourself to see that normalizing $[ 256, 128, 64 ]$ will result in $[0.87, 0.43, 0.22]$, which is the same as the normalized version of the original RGB vector. You can see that normalizing a vector removes the scale.\nThe normalization will be processed in a block size $2\\times 2$ cell (each cell size $8\\times8$ pixel). So we will have 4 vector histogram size $1\\times9$, concatenate vectors and we will have one vector histogram size $1\\times36$ and then normalized in this vector. Sliding the window will process the same as convolution in CNN with step_size = 8 pixels.\nCompute HOG features vector After normalizing the histogram vectors, we then concatenate these $1×36$ vectors into a single large vector. This becomes the HOG vector representing the entire image.\nFor example, suppose our image is divided into a grid of squares of size $16×8$ (each cell is $8×8$). The HOG computation moves $7$ steps horizontally and $15$ steps vertically. Thus, there are a total of $7×15 = 105$ patches, each corresponding to one $36$-dimensional histogram vector. Consequently, the final HOG vector will have $105×36 = 3780$ dimensions. This is a relatively large vector, which allows it to capture the image’s features quite effectively.\nVisualizing Histogram of Oriented Gradients The HOG descriptor of an image patch is usually visualized by plotting the 9×1 normalized histograms in the 8×8 cells. See image on the side. You will notice that dominant direction of the histogram captures the shape of the person, especially around the torso and legs.\nPractice We will use opencv to calculate the HOG features\nprint(\u0026#34;Original {}\u0026#34;.format(img.shape)) cell_size = (8,8) # h x w pixels block_size = (2,2) # h x w cells nbins = 9 # number of bins in Histogram # winSize = the size of the image region (or window) that will be used by the HOG (Histogram of Oriented Gradients) descriptor winSize = (img.shape[1]// cell_size[1] * cell_size[1], img.shape[0] // cell_size[0] * cell_size[0]) # blockSize = the size compute in pixels blockSize = (block_size[1] * cell_size[1], block_size[0] * cell_size[0]) # blockStride = how far (in pixels) the detection window moves from one block to the next. blockStride = cell_size print(\u0026#34;winSize\u0026#34;,winSize) print(\u0026#34;blockSize\u0026#34;,block_size) print(\u0026#34;blockStride\u0026#34;,blockStride) hog = cv2.HOGDescriptor(_winSize=winSize,_blockSize=blockSize,_blockStride=blockStride,_cellSize=cell_size,_nbins=9) # size of cell grid (from pixel -\u0026gt; cell) n_cell = (winSize[0]//cell_size[0],winSize[1]//cell_size[1]) print(\u0026#34;n_cell\u0026#34;,n_cell) hog_feature = hog.compute(img).reshape(n_cell[0] - block_size[0] + 1,n_cell[1] - block_size[1] + 1 ,block_size[0],block_size[1],nbins).transpose(1, 0, 2 , 3 , 4) print(\u0026#34;hog feature\u0026#34;, hog_feature.shape) and the results\nOriginal (395, 634, 3) winSize (632, 392) blockSize (2, 2) blockStride (8, 8) n_cell (79, 49) hog feature (48, 78, 2, 2, 9) Note: Don\u0026rsquo;t like the order of the .shape, opencv use the order width x height (instead height x width).\nWe can visualize HOG distribution\nfrom skimage import exposure from skimage import feature import cv2 import matplotlib.pyplot as plt (H, hogImage) = feature.hog(gray, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2), transform_sqrt=True, block_norm=\u0026#34;L2\u0026#34;, visualize=True) hogImage = exposure.rescale_intensity(hogImage, out_range=(0, 255)) hogImage = hogImage.astype(\u0026#34;uint8\u0026#34;) plt.imshow(hogImage) Application in HOG Human Detection To detect human in image or also video, we can use a pre-trained SVM (Support Vector Machine) model that makes predictions based on features extracted by the HOG (Histogram of Oriented Gradients) algorithm.\nImport lib from skimage.feature import hog from skimage.transform import pyramid_gaussian from skimage.io import imread import joblib from sklearn.preprocessing import LabelEncoder from sklearn.svm import LinearSVC from sklearn.metrics import classification_report, accuracy_score from sklearn.model_selection import train_test_split from skimage import color from imutils.object_detection import non_max_suppression import imutils import numpy as np import cv2 import argparse import cv2 import os import glob from PIL import Image # This will be used to read/modify images (can be done via OpenCV too) from numpy import * Define path to image base_path_test = \u0026#34;../data/human-and-non-human/test_set/test_set\u0026#34; # path for test base_path_train = \u0026#34;../data/dataset\u0026#34; # path for train # join the path pos_im_path = os.path.join(base_path_train, \u0026#34;positive\u0026#34;) neg_im_path = os.path.join(base_path_train, \u0026#34;negative\u0026#34;) #define negative image for SVM training pos_im_path_test = os.path.join(base_path_test, \u0026#34;humans\u0026#34;) # for test data neg_im_path_test = os.path.join(base_path_test, \u0026#34;non-humans\u0026#34;) # Check if there not exist path if not os.path.exists(pos_im_path): print(f\u0026#34;Error: Path does not exist - {os.path.abspath(pos_im_path)}\u0026#34;) if not os.path.exists(neg_im_path): print(f\u0026#34;Error: Path does not exist - {os.path.abspath(neg_im_path)}\u0026#34;) if not os.path.exists(pos_im_path_test): print(f\u0026#34;Error: Path does not exist - {os.path.abspath(pos_im_path_test)}\u0026#34;) if not os.path.exists(neg_im_path_test): print(f\u0026#34;Error: Path does not exist - {os.path.abspath(neg_im_path_test)}\u0026#34;) # Take the image in that path pos_im_listing = os.listdir(pos_im_path) neg_im_listing = os.listdir(neg_im_path) pos_im_listing_test = os.listdir(pos_im_path_test) neg_im_listing_test = os.listdir(neg_im_path_test) num_pos_samples = size(pos_im_listing) # simply states the total no. of images num_neg_samples = size(neg_im_listing) num_pos_test = size(pos_im_listing_test) num_neg_test = size(neg_im_listing_test) print(num_pos_samples) # prints the number value of the no.of samples in positive dataset print(num_neg_samples) print(num_pos_test) print(num_neg_test) data = [] labels = [] Compute the HOG features and label them # Putting label into positive image winSize = (64, 128) blockSize = (16, 16) blockStride = (8, 8) cellSize = (8, 8) nbins = 9 hog = cv2.HOGDescriptor(winSize, blockSize, blockStride, cellSize, nbins) for file in pos_im_listing: img_path = os.path.join(pos_im_path, file) try: img = Image.open(img_path).convert(\u0026#34;RGB\u0026#34;) # Open the file # Convert into NumPy array img = np.array(img) # Convert RGB to BGR (for OpenCV compatibility) img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR) # Resize to standard HOG size (64x128) to ensure consistency img = cv2.resize(img, (64, 128)) #Convert to grayscale gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) fd = hog.compute(gray).flatten() # Must change into 1D array (because fd return a multi-dimensional vector) if fd.shape[0] != 3780: # Expected size for 64x128 image print(f\u0026#34;Skipping {file} due to incorrect HOG feature shape: {fd.shape}\u0026#34;) continue # Skip this sample data.append(fd) labels.append(1) print(f\u0026#34;Processed training data (positive): {file}\u0026#34;) except Exception as e: print(f\u0026#34;Skipping {file} (Error: {e})\u0026#34;) # Putting label into negative image for file in neg_im_listing: img_path = os.path.join(neg_im_path, file) try: img = Image.open(img_path).convert(\u0026#34;RGB\u0026#34;) # Open the file # Convert into NumPy array img = np.array(img) # Convert RGB to BGR (for OpenCV compatibility) img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR) # Resize to standard HOG size (64x128) to ensure consistency img = cv2.resize(img, (64, 128)) #Convert to grayscale gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) fd = hog.compute(gray).flatten() # Must change into 1D array (because fd return a multi-dimensional vector) if fd.shape[0] != 3780: # Expected size for 64x128 image print(f\u0026#34;Skipping {file} due to incorrect HOG feature shape: {fd.shape}\u0026#34;) continue # Skip this sample data.append(fd) labels.append(0) print(f\u0026#34;Processed training data (negative): {file}\u0026#34;) except Exception as e: print(f\u0026#34;Skipping {file} (Error: {e})\u0026#34;) ## Testing label data_test = [] label_test = [] for file in pos_im_listing_test: img_path = os.path.join(pos_im_path_test, file) try: img = Image.open(img_path).convert(\u0026#34;RGB\u0026#34;) # Open the file # Convert into NumPy array img = np.array(img) # Convert RGB to BGR (for OpenCV compatibility) img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR) # Resize to standard HOG size (64x128) to ensure consistency img = cv2.resize(img, (64, 128)) #Convert to grayscale gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # Create a base features (if don\u0026#39;t add any parameter, opencv will understand to take a base parameter) fd = hog.compute(gray).flatten() # Must change into 1D array (because fd return a multi-dimensional vector) if fd.shape[0] != 3780: # Expected size for 64x128 image print(f\u0026#34;Skipping {file} due to incorrect HOG feature shape: {fd.shape}\u0026#34;) continue # Skip this sample data_test.append(fd) label_test.append(1) print(f\u0026#34;Processed test data (positive): {file}\u0026#34;) except Exception as e: print(f\u0026#34;Skipping {file} (Error: {e})\u0026#34;) # Putting label into negative image for file in neg_im_listing_test: img_path = os.path.join(neg_im_path_test, file) try: img = Image.open(img_path).convert(\u0026#34;RGB\u0026#34;) # Open the file # Convert into NumPy array img = np.array(img) # Convert RGB to BGR (for OpenCV compatibility) img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR) # Resize to standard HOG size (64x128) to ensure consistency img = cv2.resize(img, (64, 128)) #Convert to grayscale gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) fd = hog.compute(gray).flatten() # Must change into 1D array (because fd return a multi-dimensional vector) if fd.shape[0] != 3780: # Expected size for 64x128 image print(f\u0026#34;Skipping {file} due to incorrect HOG feature shape: {fd.shape}\u0026#34;) continue # Skip this sample data_test.append(fd) label_test.append(0) print(f\u0026#34;Processed test data (negative): {file}\u0026#34;) except Exception as e: print(f\u0026#34;Skipping {file} (Error: {e})\u0026#34;) Model testing print(\u0026#34; Constructing training/testing split...\u0026#34;) data = np.array(data, dtype=np.float32) labels = np.array(labels, dtype=np.int32) data_test = np.array(data_test, dtype=np.float32) label_test = np.array(label_test, dtype=np.int32) print(data_test.shape) print(label_test.shape) (trainData, testData, trainLabels, testLabels) = (data, data_test, labels, label_test) # Train the linear SVM print(\u0026#34; Training Linear SVM classifier...\u0026#34;) model = LinearSVC() model.fit(trainData, trainLabels) # Evaluate the classifier print(\u0026#34; Evaluating classifier on test data ...\u0026#34;) pred = model.predict(testData) # Calculate accuracy accuracy = accuracy_score(testLabels, pred) print(f\u0026#34;Accuracy: {accuracy}\u0026#34;) print(\u0026#34;Report\\n\u0026#34;,classification_report(testLabels, pred)) # Save the model joblib.dump(model, \u0026#39;SVM_HOG.pkl\u0026#39;) print(\u0026#34;SVM Model save Successfully!\u0026#34;) Constructing training/testing split... (1554, 3780) (1554,) Training Linear SVM classifier... Evaluating classifier on test data ... Accuracy: 0.8294723294723295 Report precision recall f1-score support 0 0.84 0.98 0.91 1309 1 0.00 0.00 0.00 245 accuracy 0.83 1554 macro avg 0.42 0.49 0.45 1554 weighted avg 0.71 0.83 0.76 1554 SVM Model save Successfully! Load model # Load the model by joblib model = joblib.load(\u0026#34;SVM_HOG.pkl\u0026#34;) print(\u0026#34;SVM Model load Successfully!\u0026#34;) Sliding window # Sliding window technique def sliding_window(image, stepSize, windowSize): for y in range(0, image.shape[0] - windowSize[1], stepSize): for x in range(0, image.shape[1] - windowSize[0], stepSize): yield (x, y, image[y:y + windowSize[1], x:x + windowSize[0]]) Detect people on image down_scale=1.25 window_size=(64,128) output_folder = \u0026#39;results/\u0026#39; # Detect for file in pos_im_listing_test: scale = 0 img_path = os.path.join(pos_im_path_test, file) try: img = Image.open(img_path).convert(\u0026#34;RGB\u0026#34;) # Open the file img = np.array(img) img = cv2.resize(img, (300,200)) # Debug: Check the shape of the image if img.shape[2] != 3: print(\u0026#34;Skip\u0026#34;) continue detections = [] # Using pyramid to detect the larger or smaller object (scaled the image purpose sliding window will detect different object with original cell) for resized in pyramid_gaussian(img, downscale=down_scale, channel_axis = -1): # pyramid_gaussian convert the image into [0, 1] # So we need to convert back to unit8 resized = (resized * 255).astype(np.uint8) # Convert float64 to uint8, go back [0, 255] for (x , y , window) in sliding_window(resized, stepSize = 8, windowSize = window_size): # You can adjust the stepsize # validation the window size if (window.shape[1] != window_size[0] or window.shape[0] != window_size[1]): continue # change the window to gray, easily compute HOG window = cv2.cvtColor(window, cv2.COLOR_RGB2GRAY) #Extract HOG features fds = hog.compute(window) fds = fds.reshape(1 , -1) # Make become 2d #prediction SVM pred = model.predict(fds) # print(pred) if pred == 1: if (model.decision_function(fds) \u0026gt; 0.5): # add threshold back_to_original = down_scale ** scale # When we use pyramid to shrink the image, we will give the detection window back to original size detections.append((x*back_to_original , y*back_to_original , int(window_size[0]*back_to_original), int(window_size[1]*back_to_original),model.decision_function(fds))) scale += 1 # increase the scale clone = img.copy() rects = np.array([[x,y,x + w , y + h] for (x,y,w,h,_) in detections]) sc = [score[0] for (_,_,_,_,score) in detections] sc = np.array(sc) # Apply Non-Maximum Suppression (NMS) if detections exist final_detections = non_max_suppression(rects, probs=sc, overlapThresh=0.35) #Draw bouding boxes for (x , y , w , h) in final_detections: cv2.rectangle(img, (x , y), (x + w, y + h), (0, 255, 0), 2) # Convert BGR to RGB for display in matplotlib img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # write down the result output_path = os.path.join(output_folder, file) cv2.imwrite(output_path, img_rgb) print(f\u0026#34;Processed {file} successful\u0026#34; ) except Exception as e: print(f\u0026#34;Skipping {file} (Error: {e})\u0026#34;) Before using NMS\nAfter using NMS\nDetect people on video from IPython.display import display, clear_output import yt_dlp import PIL.Image # Load YouTube video using pafy url = \u0026#39;https://youtu.be/NyLF8nHIquM\u0026#39; # Get the best video stream ydl_opts = {} with yt_dlp.YoutubeDL(ydl_opts) as ydl: info_dict = ydl.extract_info(url, download=False) video_url = info_dict[\u0026#39;url\u0026#39;] cap = cv2.VideoCapture(video_url) # Set Optimized Performance cv2.setUseOptimized(True) cv2.setNumThreads(4) # Adjust based on CPU # Process every nth frame (skip frames) frame_skip = 2 frame_count = 0 # Read and process frames while True: ret, frame = cap.read() if not ret: break if frame_count % frame_skip != 0: frame_count += 1 continue frame_count += 1 # Increment counter img = cv2.resize(frame, (512,512)) detections = [] scale = 0 # Using pyramid to detect the larger or smaller object (scaled the image purpose sliding window will detect different object with original cell) for resized in pyramid_gaussian(img, downscale=down_scale, channel_axis = -1): # pyramid_gaussian convert the image into [0, 1] # So we need to convert back to unit8 resized = (resized * 255).astype(np.uint8) # Convert float64 to uint8, go back [0, 255] for (x , y , window) in sliding_window(resized, stepSize = 8, windowSize = window_size): # You can adjust the stepsize # validation the window size if (window.shape[1] != window_size[0] or window.shape[0] != window_size[1]): continue # change the window to gray, easily compute HOG window = cv2.cvtColor(window, cv2.COLOR_RGB2GRAY) #Extract HOG features fds = hog.compute(window) fds = fds.reshape(1 , -1) # Make become 2d #prediction SVM pred = model.predict(fds) # print(pred) if pred == 1: if (model.decision_function(fds) \u0026gt; 0.5): # add threshold back_to_original = down_scale ** scale # When we use pyramid to shrink the image, we will give the detection window back to original size detections.append((x*back_to_original , y*back_to_original , int(window_size[0]*back_to_original), int(window_size[1]*back_to_original),model.decision_function(fds))) scale += 1 # increase the scale clone = img.copy() rects = np.array([[x,y,x + w , y + h] for (x,y,w,h,_) in detections]) sc = [score[0] for (_,_,_,_,score) in detections] sc = np.array(sc) # Apply Non-Maximum Suppression (NMS) if detections exist final_detections = non_max_suppression(rects, probs=sc, overlapThresh=0.35) #Draw bouding boxes for (x , y , w , h) in final_detections: cv2.rectangle(img, (x , y), (x + w, y + h), (0, 255, 0), 2) cv2.putText(img, \u0026#39;Person: {:.2f}\u0026#39;.format(np.max(sc)), (x - 2, y - 2), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1) # Convert BGR to RGB for display in matplotlib img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) img_pil = PIL.Image.fromarray(img_rgb) # Clear previous frame and display new frame clear_output(wait=True) display(img_pil) # Add a small delay and allow stopping if cv2.waitKey(1) == 27: # Press ESC to exit break cap.release() cv2.destroyAllWindows() Summary In image processing, the Histogram of Oriented Gradients (HOG) algorithm is one of the powerful feature descriptors that encodes an image into a feature vector with a sufficiently large number of dimensions to effectively classify images. The algorithm works based on representing a histogram vector of gradient magnitudes according to bins of gradient directions applied to local image regions. Normalization methods are applied to make the aggregated histogram vector invariant to changes in image intensity, ensuring consistency for images with the same content but different brightness levels.\nIn object detection, the HOG algorithm proves to be highly effective, particularly in detecting people at various scales. Additionally, in some image classification cases where the dataset is small, large neural networks such as CNNs may not perform accurately due to the training set not covering all possible variations. In such situations, applying classical feature extraction methods like HOG can yield surprisingly good results while requiring fewer computational resources and lower costs.\nThis demonstrates that although HOG is an older method, it remains highly effective in many applications. Depending on the specific scenario, we may choose to use the HOG algorithm instead of necessarily applying a deep learning model with millions of parameters to achieve high accuracy.\nReference https://phamdinhkhanh.github.io/2019/11/22/HOG.html Object Detection for Dummies Part 1: Gradient Vector, HOG, and SS | Lil\u0026rsquo;Log Histogram of Oriented Gradients explained using OpenCV https://github.com/nguyentuss/CV (full code and data put in here) ","permalink":"http://localhost:1313/posts/hog/","summary":"\u003chr\u003e\n\u003cp\u003eThere are many different methods in computer vision. When it comes to image classification, we can apply families of CNN models such as Inception Net, MobileNet, ResNet, DenseNet, AlexNet, U-Net, and so on. For object detection, models like YOLO, SSD, Faster R-CNN, Fast R-CNN, and Mask R-CNN are commonly used.\nAll the above algorithms belong to the deep learning category. However, before the deep learning boom, what algorithms were typically used in image processing? Today, we will explore a classic yet highly effective algorithm in image processing: \u003cstrong\u003eHOG (Histogram of Oriented Gradients).\u003c/strong\u003e\nThis algorithm generates \u003cstrong\u003efeatures description\u003c/strong\u003e for the purpose of \u003cstrong\u003eobject detection\u003c/strong\u003e. From an image, two key matrices are extracted to store essential information: \u003cstrong\u003egradient magnitude\u003c/strong\u003e and \u003cstrong\u003egradient orientation\u003c/strong\u003e. By combining these two pieces of information into a histogram distribution—where the gradient magnitude is counted in bins according to gradient orientation—we obtain a \u003cstrong\u003eHOG feature vector\u003c/strong\u003e that represents the histogram. This is the basic concept, but in practice, the algorithm is more complex. The \u003cstrong\u003eHOG vector\u003c/strong\u003e is computed over \u003cstrong\u003elocal regions\u003c/strong\u003e, similar to how CNNs operate, followed by \u003cstrong\u003elocal normalization\u003c/strong\u003e to standardize measurements. Finally, the overall \u003cstrong\u003eHOG vector\u003c/strong\u003e is aggregated from all local vectors.\u003c/p\u003e","title":"Histogram of Oriented Gradients (HOG)"},{"content":" Supervised Learning The task $T$ is to learn a mapping $f$ from $x \\in X$ to $y \\in Y$. The $x$ are also called the $\\mathbf{features}$. The output $y$ is called the $\\mathbf{label}$. The experience $E$ is given in the form of a set of $N$ input-output pairs $\\mathcal{D} = {(x_n,y_n)},; n = 1 \\rightarrow N$ called the $\\textbf{training set}$ (with $N$ as the $\\textbf{sample size}$). The performance $P$ depends on the type of output we want to predict.\nClassification In a classification problem, the output space is a set of $C$ labels called $\\textbf{classes}$, $Y = {1,2,\u0026hellip;,C}$. The problem of predicting the class label given an input is called $\\textbf{pattern recognition}$. The goal of supervised learning in a classification problem is to predict the label. A common way to measure performance on this task is by the $\\textbf{misclassification rate}$.\n$$\\mathcal{L}(\\boldsymbol{\\theta}) \\triangleq \\frac{1}{N} \\sum_{n=1}^{N} \\mathbb{I}\\left(y_n \\neq f(x_n; \\boldsymbol{\\theta})\\right).$$\nHere, $\\mathbb{I}(e)$ is the indicator function that returns 1 if the condition is true and 0 otherwise. We can also use the notation $\\textbf{loss function}$ $l(y,\\hat{y})$:\n$$\\mathcal{L}(\\boldsymbol{\\theta}) \\triangleq \\frac{1}{N} \\sum_{n=1}^{N} \\ell\\left(y_n, f(x_n; \\boldsymbol{\\theta})\\right).$$\nRegression In regression, the output $y \\in \\mathbb{R}$ is a real value instead of a discrete label $y \\in {1,\u0026hellip;,C}$. A common choice for the loss function is the quadratic loss, or $\\ell_2$ loss:\n$$\\ell_2(y,\\hat{y}) = (y - \\hat{y})^2.$$\nThis penalizes large residuals $y-\\hat{y}$. The empirical risk when using quadratic loss is equal to the $\\textbf{Mean Squared Error (MSE)}$:\n$$MSE(\\boldsymbol{\\theta})=\\frac{1}{N} \\sum_{n=1}^{N}(y_n-f(x_n;\\boldsymbol{\\theta}))^2.$$\nAn example of a regression model for 1D data is the $\\textbf{linear regression}$ model:\n$$f(x;\\boldsymbol{\\theta})=b+wx.$$\nFor multiple input features, we can write:\n$$f(\\mathbf{x};\\boldsymbol{\\theta})=b+w_1x_1+\\cdots+w_Dx_D=b+\\mathbf{w}^T\\mathbf{x}.$$\nWe can improve the fit by using a $\\textbf{Polynomial regression}$ model with degree $\\mathcal{D}$:\n$$f(x;\\mathbf{w})=\\mathbf{w}^T\\phi(x),$$\nwhere $\\phi(x)=[1,x,x^2,\\dots,x^D]$ is the feature vector derived from the input.\nOverfitting The empirical risk (training loss function) is given by:\n$$\\mathcal{L}(\\boldsymbol{\\theta}; \\mathcal{D}{\\text{train}}) = \\frac{1}{|\\mathcal{D}{\\text{train}}|} \\sum_{(\\mathbf{x,y}) \\in \\mathcal{D}_{\\text{train}}} \\ell(y, f(x; \\boldsymbol{\\theta})).$$\nThe difference $\\mathcal{L}( \\boldsymbol{\\theta};p^)-\\mathcal{L}(\\boldsymbol{\\theta};\\mathcal{D}_{\\text{train}})$ is called the $\\textbf{generalization gap}$. If a model has a large generalization gap (i.e., low empirical risk but high population risk), it is a sign that it is overfitting. In practice we do not know $p^$, so we partition the data into a training set and a $\\textbf{test set}$ to approximate the population risk using the $\\textbf{test risk}$:\n$$\\mathcal{L}(\\boldsymbol{\\theta}; \\mathcal{D}{\\text{test}}) = \\frac{1}{|\\mathcal{D}{\\text{test}}|} \\sum_{(\\mathbf{x,y}) \\in \\mathcal{D}_{\\text{test}}} \\ell(y, f(x; \\boldsymbol{\\theta})).$$\nWe can drive the training loss function to zero by increasing the degree $\\mathcal{D}$, but this may increase the testing loss function. A model that fits the training data too closely is said to be $\\textbf{overfitting}$, while a model that is too simple and does not capture the underlying structure is said to be $\\textbf{underfitting}$.\nUnsupervised Learning In supervised learning, we assume that each input $x$ in the training set has a corresponding target $y$, and our goal is to learn the input-output mapping. In contrast, unsupervised learning deals with data that has no output labels; the dataset is simply $\\mathcal{D} = {x_n : n = 1,\\dots,N}$. Unsupervised learning focuses on modeling the underlying structure of the data by fitting an unconditional model $p(x)$, rather than a conditional model $p(y|x)$.\nUnsupervised learning avoids the need for large labeled datasets, which can be time-consuming and expensive to collect, and instead finds patterns, structures, or groupings in the data based on inherent similarities or relationships.\nClustering A simple example of unsupervised learning is clustering, where the goal is to partition the input data into regions that contain similar points.\nSelf-supervised Learning $\\textbf{Self-supervised learning}$ automatically generates $\\textbf{labels}$ from $\\textbf{unlabeled data}$. For example, one may learn to predict a color image from its grayscale version, or mask out words in a sentence and predict them from the surrounding context. In this setting, a predictor such as\n$$\\hat{x}_1 = f(x_2;\\boldsymbol{\\theta})$$\n(where $x_2$ is the observed input and $\\hat{x}_1$ is the predicted output) learns useful features from the data that can be leveraged in standard supervised tasks.\nReinforcement Learning In reinforcement learning, an agent learns how to interact with its environment. For example, a bot playing Mario learns to interact with the game world by moving left or right and jumping when encountering obstacles. (Click to see the detail)\nPreprocessing Discrete Input Data One-hot Encoding For categorical features, we often convert them into numerical values using $\\textbf{one-hot encoding}$. For a categorical variable $x$ with $K$ possible values, one-hot encoding is defined as:\n$$\\text{one-hot}(x) = [\\mathbb{I}(x=1), \\dots, \\mathbb{I}(x=K)].$$\nFor example, if $x$ represents one of three colors (red, green, blue), then:\none-hot(red) = $[1,0,0]$ one-hot(green) = $[0,1,0]$ one-hot(blue) = $[0,0,1]$. Feature Crosses To capture interactions between categorical features, we often create composite features. Suppose we want to predict the fuel efficiency of a vehicle based on two categorical variables:\n$x_1$: The type of car (SUV, Truck, Family car). $x_2$: The country of origin (USA, Japan). Using one-hot encoding, we represent these variables as:\n$$\\phi(x) = [1, I(x_1 = S), I(x_1 = T), I(x_1 = F), I(x_2 = U), I(x_2 = J)].$$\nHowever, this encoding does not capture interactions. To capture interactions between car type and country, we define composite features:\n$$\\text{(Car type, Country)} = {(S, U), (T, U), (F, U), (S, J), (T, J), (F, J)}.$$\nThe new model becomes:\n$$f(x; w) = w^T \\phi(x).$$\nExpressing this in full:\n$$\\begin{split}f(x; w) = w_0 + w_1 I(x_1 = S) + w_2 I(x_1 = T) + w_3 I(x_1 = F) \\ + w_4 I(x_2 = U) + w_5 I(x_2 = J) + w_6 I(x_1 = S, x_2 = U) \\ + w_7 I(x_1 = T, x_2 = U) + w_8 I(x_1 = F, x_2 = U) \\ + w_9 I(x_1 = S, x_2 = J) + w_{10} I(x_1 = T, x_2 = J) + w_{11} I(x_1 = F, x_2 = J).\\end{split}$$\nSummary This post introduces key concepts in machine learning—from supervised learning (both classification and regression) to unsupervised, self-supervised, and reinforcement learning. It also covers preprocessing techniques for categorical data such as one-hot encoding and feature crosses. The provided equations are formatted in a single-line style for consistency in this Markdown document.\n","permalink":"http://localhost:1313/posts/introduction/","summary":"\u003chr\u003e\n\u003ch2 id=\"supervised-learning\"\u003eSupervised Learning\u003c/h2\u003e\n\u003cp\u003eThe task $T$ is to learn a mapping $f$ from $x \\in X$ to $y \\in Y$. The $x$ are also called the $\\mathbf{features}$. The output $y$ is called the $\\mathbf{label}$. The experience $E$ is given in the form of a set of $N$ input-output pairs $\\mathcal{D} = {(x_n,y_n)},; n = 1 \\rightarrow N$ called the $\\textbf{training set}$ (with $N$ as the $\\textbf{sample size}$). The performance $P$ depends on the type of output we want to predict.\u003c/p\u003e","title":"Introduction"},{"content":"Joint distributions for multiple random variables Covariance The covariance between two random variables ${X}$ and ${Y}$ measures the direction of the linear relationship to which ${X}$ and ${Y}$ are (linearly) related. It quantifies how the random variables change together.\nPositive: If one increases, the other also increases. Negative: If one increases while the other decreases. Zero: There is no relationship between the variables. $$\\textrm{Cov}[X,Y] \\triangleq \\mathbb{E}\\Bigl[(X-\\mathbb{E}[X])(Y-\\mathbb{E}[Y])\\Bigr] = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y].$$\nIf ${\\mathbf{x}}$ is a $D$-dimensional random vector, its covariance matrix is defined as\n$$\\textrm{Cov}[\\mathbf{x}] \\triangleq \\mathbb{E}\\Bigl[(\\mathbf{x}-\\mathbb{E}[\\mathbf{x}])(\\mathbf{x}-\\mathbb{E}[\\mathbf{x}])^T\\Bigr] \\triangleq \\mathbf{\\Sigma} =.$$\n$$\\begin{pmatrix} \\textrm{Var}[X_1] \u0026amp; \\textrm{Cov}[X_1,X_2] \u0026amp; \\cdots \u0026amp; \\textrm{Cov}[X_1,X_D] \\ \\textrm{Cov}[X_2,X_1] \u0026amp; \\textrm{Var}[X_2] \u0026amp; \\cdots \u0026amp; \\textrm{Cov}[X_2,X_D] \\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ \\textrm{Cov}[X_D,X_1] \u0026amp; \\textrm{Cov}[X_D,X_2] \u0026amp; \\cdots \u0026amp; \\textrm{Var}[X_D] \\end{pmatrix}$$\nCovariance itself is the variance of the distribution, from which we can get the important result\n$$\\mathbb{E}[\\mathbf{x}\\mathbf{x}^T] = \\mathbf{\\Sigma} + \\mathbf{\\mu}\\mathbf{\\mu}^T.$$\nAnother useful result is that the covariance of a linear transformation\n$$\\textrm{Cov}[\\mathbf{A}\\mathbf{x} + b] = \\mathbf{A} , \\textrm{Cov}[\\mathbf{x}] , \\mathbf{A}^T.$$\nThe cross-covariance between two random vectors is defined by\n$$\\textrm{Cov}[\\mathbf{x},\\mathbf{y}] = \\mathbb{E}\\Bigl[(\\mathbf{x}-\\mathbb{E}[\\mathbf{x}])(\\mathbf{y}-\\mathbb{E}[\\mathbf{y}])^T\\Bigr].$$\nCorrelation Covariance can range over all real numbers. Sometimes it is more convenient to work with a normalized measure that is bounded. The correlation coefficient between ${X}$ and ${Y}$ is defined as\n$$\\rho \\triangleq \\textrm{Corr}[X,Y] \\triangleq \\frac{\\textrm{Cov}[X,Y]}{\\sqrt{\\textrm{Var}[X]\\textrm{Var}[Y]}}.$$\nWhile covariance can be any real number, correlation is always between ${-1}$ and ${1}$. In the case of a vector ${\\mathbf{x}}$ of related variables, the correlation matrix is given by\n$$\\textrm{Corr}(\\mathbf{x}) = \\begin{pmatrix} 1 \u0026amp; \\frac{\\mathbb{E}[(X_1 - \\mu_1)(X_2 - \\mu_2)]}{\\sigma_1 \\sigma_2} \u0026amp; \\cdots \u0026amp; \\frac{\\mathbb{E}[(X_1 - \\mu_1)(X_D - \\mu_D)]}{\\sigma_1 \\sigma_D} \\ \\frac{\\mathbb{E}[(X_2 - \\mu_2)(X_1 - \\mu_1)]}{\\sigma_2 \\sigma_1} \u0026amp; 1 \u0026amp; \\cdots \u0026amp; \\frac{\\mathbb{E}[(X_2 - \\mu_2)(X_D - \\mu_D)]}{\\sigma_2 \\sigma_D} \\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ \\frac{\\mathbb{E}[(X_D - \\mu_D)(X_1 - \\mu_1)]}{\\sigma_D \\sigma_1} \u0026amp; \\frac{\\mathbb{E}[(X_D - \\mu_D)(X_2 - \\mu_2)]}{\\sigma_D \\sigma_2} \u0026amp; \\cdots \u0026amp; 1 \\end{pmatrix}.$$\nNote that uncorrelated does not imply independent. For example, if ${X}\\sim \\textrm{Unif}(-1,1)$ and ${Y} = {X}^2$, even though $\\textrm{Cov}[X,Y]=0$ and $\\textrm{Corr}[X,Y]=0$, ${Y}$ clearly depends on ${X}$.\nSimpson Paradox Simpson\u0026rsquo;s paradox demonstrates that a statistical trend observed in several different groups of data can disappear or even reverse when these groups are combined.\nThe Multivariate Gaussian (Normal) Distribution The multivariate Gaussian (normal) distribution generalizes the univariate Gaussian to multiple dimensions.\n$$\\mathcal{N}(\\mathbf{y};\\mathbf{\\mu},\\mathbf{\\Sigma}) \\triangleq \\frac{1}{(2\\pi)^{D/2}|\\mathbf{\\Sigma}|^{1/2}}\\exp\\left(-\\frac{1}{2}(\\mathbf{y}-\\mathbf{\\mu})^T\\mathbf{\\Sigma}^{-1}(\\mathbf{y}-\\mathbf{\\mu})\\right).$$\nwhere ${\\mathbf{\\mu}} = \\mathbb{E}[\\mathbf{y}] \\in \\mathbb{R}^D$ is the mean vector and ${\\mathbf{\\Sigma}} = \\textrm{Cov}[\\mathbf{y}]$ is the $D\\times D$ covariance matrix.\nIn 2D, the MVN is known as the Bivariate Gaussian distribution. In this case, if ${\\mathbf{y}} \\sim \\mathcal{N}(\\mathbf{\\mu},\\mathbf{\\Sigma})$ with\n$$\\mathbf{\\Sigma} = \\begin{pmatrix} \\sigma_1^2 \u0026amp; \\rho,\\sigma_1,\\sigma_2 \\ \\rho,\\sigma_1,\\sigma_2 \u0026amp; \\sigma_2^2 \\end{pmatrix},$$\nthe marginal distribution ${p(y_1)}$ is a 1D Gaussian given by\n$$p(y_1) = \\mathcal{N}(y_1 \\mid \\mu_1, \\sigma_1^2).$$\nand if we observe ${y_2}$, the conditional distribution is\n$$p(y_1 \\mid y_2) = \\mathcal{N}!\\Biggl(y_1 \\Bigl| \\mu_1 + \\frac{\\rho,\\sigma_1,\\sigma_2}{\\sigma_2^2}(y_2 - \\mu_2),, \\sigma_1^2 - \\frac{(\\rho,\\sigma_1,\\sigma_2)^2}{\\sigma_2^2} \\Bigr.\\Biggr).$$\nIf $\\sigma_1 = \\sigma_2 = \\sigma$, then\n$$p(y_1 \\mid y_2) = \\mathcal{N}!\\Biggl(y_1 \\Bigl| \\mu_1 + \\rho(y_2 - \\mu_2),, \\sigma^2(1 - \\rho^2) \\Bigr.\\Biggr).$$\nFor instance, if ${\\rho = 0.8}$, ${\\sigma_1 = \\sigma_2 = 1}$, ${\\mu_1 = \\mu_2 = 0}$, and ${y_2} = 1$, then $\\mathbb{E}[y_1 \\mid y_2 = 1] = 0.8$ and\n$$\\textrm{Var}(y_1 \\mid y_2 = 1) = 1 - 0.8^2 = 0.36.$$\nMarginals and Conditionals of an MVN Suppose ${\\mathbf{y}} = (y_1, y_2)$ is jointly Gaussian with parameters\n$$\\mathbf{\\mu} = \\begin{pmatrix} \\mu_1 \\ \\mu_2 \\end{pmatrix}, \\quad \\mathbf{\\Sigma} = \\begin{pmatrix} \\Sigma_{11} \u0026amp; \\Sigma_{12} \\ \\Sigma_{21} \u0026amp; \\Sigma_{22} \\end{pmatrix}.$$\nThen the marginals are\n$$p(y_1) = \\mathcal{N}(y_1 \\mid \\mu_1, \\Sigma_{11}), \\qquad p(y_2) = \\mathcal{N}(y_2 \\mid \\mu_2, \\Sigma_{22}).$$\nand the conditional is\n$$p(y_1 \\mid y_2) = \\mathcal{N}(y_1 \\mid \\mu_{1|2}, \\Sigma_{1|2})$$ $$\\quad \\mu_{1|2} = \\mu_1 + \\Sigma_{12}\\Sigma_{22}^{-1}(y_2 - \\mu_2), \\quad \\Sigma_{1|2} = \\Sigma_{11} -\\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21}.$$\nExample: Missing Data Suppose we sample ${N=10}$ vectors from an 8D Gaussian and then hide 50% of the data. For each example ${n}$, let ${v}$ denote the indices of the visible features and ${h}$ the hidden ones. With model parameters ${\\theta = (\\mathbf{\\mu}, \\mathbf{\\Sigma})}$, we compute the marginal distribution\n$$p(\\mathbf{y}{n,h}\\mid \\mathbf{y}{n,v}, \\theta)$$\nfor each missing variable ${i \\in h}$, and then set the prediction as the posterior mean\n$$\\bar{y}{n,i} = \\mathbb{E}!\\Bigl[ y{n,i} \\mid \\mathbf{y}_{n,v}, \\theta \\Bigr].$$\nThe posterior variance\n$$\\textrm{Var}!\\Bigl[y_{n,i} \\mid \\mathbf{y}_{n,v}, \\theta\\Bigr]$$\nindicates our confidence in that prediction.\nLinear Gaussian Systems Consider a scenario where ${z\\in \\mathbb{R}^L}$ is an unknown value and ${y\\in \\mathbb{R}^D}$ is a noisy measurement of ${z}$. Assume\n$$\\begin{aligned} p(z) \u0026amp;= \\mathcal{N}(z \\mid \\mu_z, \\Sigma_z), \\qquad p(y \\mid z) = \\mathcal{N}(y \\mid Wz + b, \\Sigma_y), \\end{aligned}$$\nwhere ${W}$ is a ${D\\times L}$ matrix. The joint distribution ${p(z,y) = p(z)p(y\\mid z)}$ is an $(L+D)$-dimensional Gaussian with mean\n$$\\mu = \\begin{pmatrix} \\mu_z \\ W\\mu_z + b \\end{pmatrix},$$\nand covariance\n$$\\Sigma = \\begin{pmatrix} \\Sigma_z \u0026amp; \\Sigma_zW^T \\ W\\Sigma_z \u0026amp; \\Sigma_y + W\\Sigma_zW^T \\end{pmatrix}.$$\nBayes Rule for Gaussians The posterior distribution is\n$$\\begin{aligned} p(z \\mid y) \u0026amp;= \\mathcal{N}(z \\mid \\mu_{z \\mid y}, \\Sigma_{z \\mid y}), \\quad \\Sigma_{z \\mid y}^{-1} = \\Sigma_z^{-1} + W^T\\Sigma_y^{-1}W, \\quad \\mu_{z \\mid y} = \\Sigma_{z \\mid y}\\Bigl[ W^T\\Sigma_y^{-1}(y - b) + \\Sigma_z^{-1}\\mu_z \\Bigr]. \\end{aligned}$$\nThe normalization constant is given by\n$$\\begin{aligned} p(y) \u0026amp;= \\int \\mathcal{N}(z \\mid \\mu_z, \\Sigma_z),\\mathcal{N}(y \\mid Wz+b, \\Sigma_y),dz = \\mathcal{N}\\Bigl(y \\mid W\\mu_z+b,, \\Sigma_y+W\\Sigma_zW^T\\Bigr). \\end{aligned}$$\nDerivation The log of the joint distribution is\n$$\\begin{aligned} \\log p(z,y) \u0026amp;= -\\frac{1}{2}(z-\\mu_z)^T\\Sigma_z^{-1}(z-\\mu_z) -\\frac{1}{2}(y-Wz-b)^T\\Sigma_y^{-1}(y-Wz-b). \\end{aligned}$$\nThis quadratic form can be rearranged (by completing the square) to derive the expressions for ${\\Sigma_{z \\mid y}}$ and ${\\mu_{z \\mid y}}$.\nExample: Inferring an Unknown Scalar Suppose we make ${N}$ noisy measurements ${y_i}$ of a scalar ${z}$ with measurement noise precision ${\\lambda_y = \\frac{1}{\\sigma^2}}$:\n$$p(y_i \\mid z) = \\mathcal{N}(y_i \\mid z, \\lambda_y^{-1}),$$\nand assume a prior\n$$p(z) = \\mathcal{N}(z \\mid \\mu_0, \\lambda_0^{-1}).$$\nThen the posterior is\n$$p(z \\mid y_1,\\dots,y_N) = \\mathcal{N}(z \\mid \\mu_N, \\lambda_N^{-1}),$$\nwith\n$$\\lambda_N = \\lambda_0 + N\\lambda_y,$$\nand\n$$\\mu_N = \\frac{N\\lambda_y\\overline{y} + \\lambda_0\\mu_0}{\\lambda_N}.$$\nIn other words, the posterior mean is a weighted average of the prior mean and the sample mean. The posterior variance is\n$$\\tau_N^2 = \\frac{\\sigma^2,\\tau_0^2}{N\\tau_0^2 + \\sigma^2},$$\nwhich decreases as more data is observed.\nSequential updates follow the same principle. After observing ${y_1}$:\n$$p(z \\mid y_1) = \\mathcal{N}(z \\mid \\mu_1, \\sigma_1^2),$$\nwith\n$$\\mu_1 = \\frac{\\sigma_y^2\\mu_0 + \\sigma_0^2y_1}{\\sigma_0^2 + \\sigma_y^2}, \\quad \\sigma_1^2 = \\frac{\\sigma_0^2\\sigma_y^2}{\\sigma_0^2 + \\sigma_y^2}.$$\nThen, using ${p(z \\mid y_1)}$ as the new prior, subsequent updates follow similarly.\nSignal-to-noise Ratio (SNR):\n$$\\text{SNR} = \\frac{\\mathbb{E}[Z^2]}{\\mathbb{E}[\\epsilon^2]} = \\frac{\\Sigma_0 + \\mu_0^2}{\\Sigma_y}.$$\nThis ratio indicates how much the data refines our estimate of ${z}$.\nMixture Models ","permalink":"http://localhost:1313/posts/multivariate-models/","summary":"\u003ch2 id=\"joint-distributions-for-multiple-random-variables\"\u003eJoint distributions for multiple random variables\u003c/h2\u003e\n\u003ch3 id=\"covariance\"\u003eCovariance\u003c/h3\u003e\n\u003cp\u003eThe \u003cstrong\u003ecovariance\u003c/strong\u003e between two random variables ${X}$ and ${Y}$ measures the \u003cstrong\u003edirection\u003c/strong\u003e of the \u003cstrong\u003elinear relationship\u003c/strong\u003e to which ${X}$ and ${Y}$ are (linearly) related. It quantifies how the random variables change together.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e  Positive: If one increases, the other also increases.\u003c/li\u003e\n\u003cli\u003e  Negative: If one increases while the other decreases.\u003c/li\u003e\n\u003cli\u003e  Zero: There is no relationship between the variables.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e$$\\textrm{Cov}[X,Y] \\triangleq \\mathbb{E}\\Bigl[(X-\\mathbb{E}[X])(Y-\\mathbb{E}[Y])\\Bigr] = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y].$$\u003c/p\u003e","title":"Multivariate Models"},{"content":"Introduction Calling sample space ${\\mathcal{X}}$ are all possible experiences, and an event will be a subset of the sample space.\nUnion $$Pr(A \\land B)=Pr(A,B).$$\nIf independent events,\n$$Pr(A \\land B)=Pr(A)Pr(B).$$\nWe say a set of variables ${X_1, \\dots, X_n}$ is (mutually) independent if the joint can be written as a product of marginals for all subsets ${{X_1,\\dots, X_m} \\subseteq {X_1,\\dots,X_n}}$,\n$$p(X_1,X_2,\\dots,X_n)=\\prod_{i=1}^{m}p(X_i).$$\nDisjoint $$Pr(A \\vee B)=Pr(A)+Pr(B)-Pr(A\\land B).$$\nConditional Probability $$Pr(B|A)\\triangleq \\frac{Pr(A,B)}{Pr(A)}.$$\nIf events ${A}$ and ${B}$ are conditionally independent given event ${C}$,\n$$Pr(A,B|C)=Pr(A|C)Pr(B|C).$$\nBe careful, we say ${X_1, X_2, X_3}$ are mutually independent if the following conditions hold:\n$$\\begin{split} p(X_1,X_2,X_3)=p(X_1)p(X_2)p(X_3),\\, p(X_1,X_2)=p(X_1)p(X_2),\\, p(X_1,X_3)=p(X_1)p(X_3),\\, p(X_2,X_3)=p(X_2)p(X_3) \\end{split}.$$\nRandom Variables Given an experiment with sample space ${\\mathbb{S}}$, a random variable (r.v.) is a function mapping from ${\\mathbb{S}}$ to ${\\mathbb{R}}$.\nDiscrete Random Variables If the sample space ${\\mathbb{S}}$ is finite or countable, it is called a discrete r.v. Denote the probability of events in ${\\mathbb{S}}$ having value ${x}$ by ${Pr(X=x)}$. This is called the probability mass function (pmf):\n$$p(x)\\triangleq Pr(X=x).$$\nThe pmf satisfies ${0\\leq p(x) \\leq 1}$ and ${\\sum_{x\\in \\mathcal{X}}p(x)=1}$.\nContinuous Random Variables If ${X \\in \\mathbb{R}}$, it is called a continuous r.v. The values no longer create a finite set of distinct possible values.\nCumulative Distribution Function (cdf) $$P(x) \\triangleq Pr(X\\leq x).$$\nWe can compute the probability of any interval:\n$$P(a\\leq x \\leq b) = P(b)-P(a-1).$$\nIn discrete r.v, the cdf will compute:\n$$P(x)=\\sum_{x\\in \\mathcal{X}}p(x).$$\nIn continuous r.v, the cdf will compute:\n$$P(x)=\\int_{x\\in \\mathcal{X}}p(x).$$\nProbability Density Function (pdf) Define the pdf as the derivative of the cdf:\n$$p(x) \\triangleq \\frac{d}{dx}P(x).$$\nAs the size of the interval gets smaller, we can write:\n$$ Pr(x \u0026lt; X \u0026lt; x + dx) \\approx p(x)dx $$\nQuantiles If the cdf ${P}$ is monotonically increasing, it has an inverse called the inverse cdf. If ${P}$ is the cdf of ${X}$, then ${P^{-1}(q)}$ is the value ${x_q}$ such that ${Pr(X\\leq x_q)=q}$; this is called the q\u0026rsquo;th quantile of ${P}$.\nSets of Related Random Variables Suppose we have two r.v. ${X}$ and ${Y}$. We can define the joint distribution:\n$$p(x,y)=Pr(X=x,Y=y),$$\nfor all possible values of ${x}$ and ${y}$. We can represent all possible values by a 2D table. For example:\n$$\\begin{array}{c|cc} p(X,Y) \u0026amp; Y = 0 \u0026amp; Y = 1 \\ \\hline X = 0 \u0026amp; 0.2 \u0026amp; 0.3 \\ X = 1 \u0026amp; 0.3 \u0026amp; 0.2 \\end{array}.$$\nHere, ${Pr(X=0,Y=1)=0.3}$, and\n$$\\sum_{x \\in \\mathcal{X},y \\in \\mathcal{Y}}p(x,y)=1.$$\nMoments of a Distribution The mean (or expected value) for a continuous r.v. is defined as:\n$$\\mathbb{E}[X]=\\int_{x\\in \\mathcal{X}}x,p(x)dx.$$\nFor discrete r.v, the mean is:\n$$\\mathbb{E}[X]=\\sum_{x\\in \\mathcal{X}}x,p(x).$$\nSince the mean is linear, we have the linearity of expectation:\n$$\\mathbb{E}[aX+b]=a,\\mathbb{E}[X]+b.$$\nFor ${n}$ random variables, the sum of expectations is:\n$$\\mathbb{E}\\Bigl[\\sum X_i\\Bigr]=\\sum \\mathbb{E}[X_i].$$\nIf they are independent, the expectation of the product is:\n$$\\mathbb{E}\\Bigl[\\prod X_i\\Bigr]=\\prod \\mathbb{E}[X_i].$$\nWhen dealing with two or more dependent r.v\u0026rsquo;s, we can compute the moment of one given the others:\n$$\\mathbb{E}[X]=\\mathbb{E}_Y\\Bigl[\\mathbb{E}[X|Y]\\Bigr].$$\nA similar formula exists for the variance:\n$$\\mathbb{V}[X]=\\mathbb{E}_Y\\Bigl[\\mathbb{V}[X|Y]\\Bigr] +\\mathbb{V}_Y\\Bigl[\\mathbb{E}[X|Y]\\Bigr].$$\nThe variance is a measure of how \u0026ldquo;spread out\u0026rdquo; the distribution is, denoted as ${\\sigma^2}$ and defined as:\n$$\\mathbb{V}[X] \\triangleq \\mathbb{E}[(X- \\mu)^2] =\\int (x-\\mu)^2p(x)dx =\\mathbb{E}[X^2]-\\mu^2.$$\nThe standard deviation is given by:\n$$std[X]=\\sqrt{\\mathbb{V}[X]}=\\sigma.$$\nLower deviation means the distribution is closer to the mean; higher deviation means it is further away.\nThe variance of a shifted and scaled version of a random variable is:\n$$\\mathbb{V}[aX+b]=a^2\\mathbb{V}[X].$$\nFor ${n}$ independent random variables, the variance of their sum is:\n$$\\mathbb{V}\\Bigl[\\sum X_i\\Bigr]=\\sum \\mathbb{V}[X_i].$$\nThe variance of their product is:\n$$\\mathbb{V}\\Bigl[\\prod X_i\\Bigr]= \\prod\\Bigl(\\sigma_i^2 + \\mu_i^2\\Bigr)-\\prod \\mu_i^2.$$\nMode of a Distribution\nThe mode of a distribution is the value with the highest probability mass or density:\n$$\\mathbf{x^*}= \\arg \\max_{\\mathbf{x}} p(\\mathbf{x}).$$\nFor multimodal distributions, the mode may not be unique.\nBayes\u0026rsquo; Rule Bayes\u0026rsquo; Rule, and with Extra Conditioning $$P({ A=a}|{ B=b}) = \\frac{P({ B=a}|{ A=b})P({ A=a})}{P({ B=b})}.$$\n$$P({ A=a}|{ B=b}, { C=c}) = \\frac{P({ B=b}|{ A=a}, { C=c})P({ A=a} \\mid { C=c})}{P({ B=b} \\mid { C=c})}.$$\nThe term ${p(A)}$ represents what we know about the possible values of ${A}$ before observing any data; this is the prior distribution. (If ${A}$ has ${K}$ possible values, then ${p(A)}$ is a vector of ${K}$ probabilities summing to ${1}$.) The term ${p(B \\mid A = a)}$ represents the distribution over possible outcomes of ${B}$ if ${A = a}$; this is the observation distribution. Evaluated at the observed ${b}$, the function ${p(B = b \\mid A = a)}$ is called the likelihood.\nMultiplying the prior ${p(A = a)}$ by the likelihood ${p(B = b \\mid A = a)}$ for each ${a}$ gives the unnormalized joint distribution ${p(A = a, B = b)}$. Normalizing by dividing by ${p(B = b)}$ (the marginal likelihood) gives:\n$$p(B = b) = \\sum_{a\u0026rsquo; \\in \\mathcal{A}} p(A = a\u0026rsquo;) p(B = b \\mid A = a\u0026rsquo;) = \\sum_{a\u0026rsquo; \\in \\mathcal{A}} p(A = a\u0026rsquo;, B = b).$$\nOdds Form of Bayes\u0026rsquo; Rule\n$$\\frac{P({ A}| { B})}{P({ A^c}| { B})} = \\frac{P({ B}|{ A})}{P({ B}| { A^c})}\\frac{P({ A})}{P({ A^c})}.$$\nThe posterior odds of ${A}$ equal the likelihood ratio times the prior odds.\nBernoulli and Binomial Distributions For an experiment tossing a coin with head probability ${0\\leq\\theta \\leq 1}$, let ${Y = 1}$ denote heads and ${Y = 0}$ denote tails. So, ${p(Y=1)=\\theta}$ and ${p(Y=0)=1-\\theta}$. This is the Bernoulli distribution, written as:\n$$Y \\sim Ber(\\theta).$$\nThe pmf is defined as:\n$$\\text{Ber}(y \\mid \\theta) = \\begin{cases} 1 - \\theta \u0026amp; \\text{if } y = 0, \\ \\theta \u0026amp; \\text{if } y = 1 \\end{cases}.$$\nIt can also be written as:\n$$\\text{Ber}(y \\mid \\theta) \\triangleq \\theta^y(1-\\theta)^{1-y}.$$\nA Bernoulli trial is a special case of the Binomial distribution. Tossing a coin ${N}$ times gives a set of ${N}$ Bernoulli trials, denoted ${y_n \\sim Ber(\\cdot\\mid\\theta)}$. Let ${s = \\sum_{n=1}^{N}\\mathbb{I}(y_n = 1)}$ be the number of heads. Then, ${s}$ follows a binomial distribution:\n$$Bin(s \\mid\\theta,N) \\triangleq \\binom{N}{s}\\theta^s(1-\\theta)^{N-s}.$$\nWhen predicting a binary variable ${y \\in {0, 1}}$ given inputs ${x \\in X}$, we use a conditional distribution of the form:\n$$p(y|\\mathbf{x},\\theta) = Ber(y\\mid f(\\mathbf{x};\\theta)).$$\nTo ensure ${0\\leq f(\\mathbf{x};\\theta)\\leq1}$, we often write:\n$$p(y|\\mathbf{x},\\theta) = Ber(y\\mid \\sigma (f(\\mathbf{x};\\theta))).$$\nwhere ${\\sigma(\\cdot)}$ is the sigmoid (or logistic) function, defined as:\n$$\\sigma(a) \\triangleq \\frac{1}{1+e^{-a}}= \\frac{e^a}{1 + e^a}.$$\nIts inverse is the logit function:\n$$a = logit(p) =\\sigma^{-1}(p) \\triangleq \\log\\frac{p}{1-p}.$$\nThus, the sigmoid transforms a function from ${\\mathbb{R}}$ into a probability in ${[0,1]}$, while the logit transforms a probability into a real number.\nSome useful properties of these functions:\n$$\\sigma(x) \\triangleq \\frac{1}{1 + e^{-x}} = \\frac{e^x}{1 + e^x}.$$\n$$\\frac{d}{dx} \\sigma(x) = \\sigma(x)(1 - \\sigma(x)).$$\n$$1 - \\sigma(x) = \\sigma(-x).$$\n$$\\sigma_+(x) \\triangleq \\log(1 + e^x) \\triangleq \\text{softplus}(x).$$\n$$\\frac{d}{dx} \\sigma_+(x) = \\sigma(x).$$\nIn particular, note an issue often encountered with activation functions: obtaining zero (or near zero) gradient during backpropagation.\nWhen ${x = 0}$:\n$$f(0) = \\frac{1}{1 + e^0} = \\frac{1}{2},$$\n$$f\u0026rsquo;(0) = \\frac{1}{2} \\cdot \\Bigl(1 - \\frac{1}{2}\\Bigr) = \\frac{1}{4} = 0.25.$$\nWhen ${x \\gg 0}$ (large positive):\n$$f(x) \\to 1, \\quad f\u0026rsquo;(x) = 1 \\cdot (1 - 1) = 0.$$\nGradient approaches 0.\nWhen ${x \\ll 0}$ (large negative):\n$$f(x) \\to 0, \\quad f\u0026rsquo;(x) = 0 \\cdot (1 - 0) = 0.$$\nGradient also approaches 0.\nCategorical and Multinomial Distributions For more than 2 classes, the categorical distribution represents a distribution over a finite set of labels ${y \\in {1,\\dots,C}}$, generalizing the Bernoulli to ${C \u0026gt; 2}$. Its pmf is:\n$$\\text{Cat}(y \\mid \\theta) \\triangleq \\prod_{c=1}^{C} \\theta_c^{\\mathbb{I}(y=c)}.$$\nIn other words, ${p(y=c \\mid \\theta)=\\theta_c}$, where the parameters are constrained so that ${0\\leq\\theta_c\\leq1}$.\nBy converting ${y}$ into a one-hot vector with ${C}$ elements (e.g. for ${C=3}$, the classes are encoded as ${ (1,0,0) }$, ${ (0,1,0) }$, and ${ (0,0,1) }$), we can view a fair 6-sided die with outcomes:\n$$y \\in {1, 2, 3, 4, 5, 6},$$\nwith each face having equal probability:\n$$\\theta = \\bigl(\\tfrac{1}{6}, \\tfrac{1}{6}, \\tfrac{1}{6}, \\tfrac{1}{6}, \\tfrac{1}{6}, \\tfrac{1}{6}\\bigr).$$\nIf we roll a 3, the one-hot encoding is:\n$$(0, 0, 1, 0, 0, 0),$$\nand\n$$P(y = 3 \\mid \\theta) = \\theta_3 = \\tfrac{1}{6}.$$\nFor a biased die with:\n$$\\theta = (0.10, 0.15, 0.30, 0.20, 0.15, 0.10),$$\nthe probability of rolling a 3 becomes:\n$$P(y = 3 \\mid \\theta) = \\theta_3 = 0.30.$$\nThe categorical distribution is a special case of the multinomial distribution. If we observe ${N}$ categorical trials, ${y_n \\sim Cat(\\cdot\\mid\\theta)}$ for ${n = 1,\\dots,N}$, and define ${y_c = \\sum_{n=1}^{N}\\mathbb{I}(y_n = c)}$, then the vector ${\\mathbf{y}}$ follows:\n$$\\mathcal{M}(\\mathbf{y} \\mid N, \\theta) \\triangleq \\binom{N}{y_1 \\dots y_C} \\prod_{c=1}^{C} \\theta_c^{y_c}.$$\nIn the conditional case, we can define:\n$$p(y \\mid x, \\theta) = \\text{Cat}(y \\mid f(x; \\theta)),$$\nor equivalently:\n$$p(y \\mid x, \\theta) = \\mathcal{M}(y \\mid 1, f(x; \\theta)),$$\nrequiring that ${0 \\leq f_c(x; \\theta) \\leq 1}$ and ${\\sum_{c=1}^{C} f_c(x; \\theta) = 1}$.\nTo avoid forcing ${f}$ to directly predict a probability vector, it is common to pass its output into the softmax function (or multinomial logit), defined as:\n$$softmax(\\mathbf{a})\\triangleq \\begin{bmatrix} \\frac{e^{a_1}}{\\sum_{c\u0026rsquo;=1}^{C}e^{a_{c\u0026rsquo;}}}, \\dots, \\frac{e^{a_C}}{\\sum_{c\u0026rsquo;=1}^{C}e^{a_{c\u0026rsquo;}}} \\end{bmatrix}.$$\nThis converts ${\\mathbb{R}^C}$ into a probability vector in ${[0,1]^C}$. One weakness is that if\n$$p_c=\\frac{e^{a_c}}{Z(\\mathbf{a})}=\\frac{e^{a_c}}{\\sum_{c\u0026rsquo;=1}^{C}e^{a_{c\u0026rsquo;}}},$$\nfor logits ${\\mathbf{a}=f(\\mathbf{x},\\theta)}$, then for extreme values like ${\\mathbf{a}=(1000,1000,1000)}$ or ${\\mathbf{a}=(-1000,-1000,-1000)}$, numerical issues (overflow or underflow) occur. To avoid this, we use the trick:\n$$\\log\\sum_{c\u0026rsquo;=1}^{C}e^{a_{c\u0026rsquo;}} = m +\\log\\sum_{c\u0026rsquo;=1}^{C}e^{a_{c\u0026rsquo;}-m},$$\nwith ${m=\\max_c a_c}$.\nThus, we have:\n$$\\begin{aligned} p(y=c|\\mathbf{x}) \u0026amp;= \\frac{\\exp(a_c-m)}{\\sum_{c\u0026rsquo;} \\exp(a_{c\u0026rsquo;}-m)} \\ \u0026amp;=e^{a_c-lse(\\mathbf{a})}. \\end{aligned}$$\nThe loss for the softmax function is given by:\n$$J(\\mathbf{W}; \\mathbf{x}, \\mathbf{y}) = - \\sum_{i=1}^{N} \\sum_{j=1}^{C} y_{ji} \\log (a_{ji}),$$\nor equivalently:\n$$J(\\mathbf{W}; \\mathbf{x}, \\mathbf{y}) = - \\sum_{i=1}^{N} \\sum_{j=1}^{C} y_{ji} \\log \\Bigl( \\frac{\\exp(\\mathbf{W}_j^T \\mathbf{x}i)}{\\sum{k=1}^{C} \\exp(\\mathbf{W}_k^T \\mathbf{x}_i)} \\Bigr).$$\nFor a single data point $({\\mathbf{x}_i}, {\\mathbf{y}_i})$, the loss is:\n$$J_i(\\mathbf{W}) \\triangleq J(\\mathbf{W}; \\mathbf{x}i, \\mathbf{y}i) = - \\sum{j=1}^{C} y{ji} \\log \\Bigl( \\frac{\\exp(\\mathbf{W}_j^T \\mathbf{x}i)}{\\sum{k=1}^{C} \\exp(\\mathbf{W}_k^T \\mathbf{x}_i)} \\Bigr).$$\nWhich can be rewritten as:\n$$J_i(\\mathbf{W}) = - \\sum_{j=1}^{C} y_{ji},\\mathbf{W}_j^T \\mathbf{x}i + \\log \\Bigl( \\sum{k=1}^{C} \\exp(\\mathbf{W}_k^T \\mathbf{x}_i) \\Bigr).$$\nThe gradient for each column ${j}$ is computed as:\n$$\\frac{\\partial J_i(\\mathbf{W})}{\\partial \\mathbf{W}j} = - y{ji},\\mathbf{x}_i + \\frac{\\exp(\\mathbf{W}_j^T \\mathbf{x}_i),\\mathbf{x}i}{\\sum{k=1}^{C} \\exp(\\mathbf{W}_k^T \\mathbf{x}_i)}.$$\nThis simplifies to:\n$$\\frac{\\partial J_i(\\mathbf{W})}{\\partial \\mathbf{W}j} = \\mathbf{x}i,(a{ji} - y{ji}),$$\nor equivalently:\n$$\\frac{\\partial J_i(\\mathbf{W})}{\\partial \\mathbf{W}j} = \\mathbf{x}i,e{ji}, \\quad \\text{where } e{ji} = a_{ji} - y_{ji}.$$\nCollecting for all columns, we have:\n$$\\frac{\\partial J_i(\\mathbf{W})}{\\partial \\mathbf{W}} = \\mathbf{x}i,[e{i1}, e_{i2}, \\dots, e_{iC}] = \\mathbf{x}_i,\\mathbf{e}_i^T,$$\nand thus the full gradient is:\n$$\\frac{\\partial J(\\mathbf{W})}{\\partial \\mathbf{W}} = \\sum_{i=1}^{N} \\mathbf{x}_i,\\mathbf{e}_i^T = \\mathbf{x},\\mathbf{E}^T,$$\nwhere ${\\mathbf{E} = \\mathbf{A} - \\mathbf{Y}}$. This compact gradient expression is useful for Batch Gradient Descent, Stochastic Gradient Descent (SGD), and Mini-batch Gradient Descent.\nAssuming SGD, the weight matrix ${\\mathbf{W}}$ is updated as:\n$$\\mathbf{W} = \\mathbf{W} + \\eta,\\mathbf{x}_i,(y_i - a_i)^T.$$\nUnivariate Gaussian (Normal) Distribution The cdf of the Gaussian is defined by:\n$$\\phi(y;\\mu,\\sigma^2)\\triangleq\\int_{-\\infty}^{y}\\mathcal{N}(z,\\mu,\\sigma^2)dz.$$\nIt can be implemented using:\n$$\\phi(y;\\mu,\\sigma^2)=\\tfrac{1}{2}\\bigl[1+erf\\bigl(\\tfrac{z}{\\sqrt{2}}\\bigr)\\bigr],$$\nwhere ${z=(y-\\mu)/\\sigma}$ and the error function is defined as:\n$$erf(u)\\triangleq\\frac{2}{\\sqrt{\\pi}}\\int_{0}^{u}e^{-t}dt.$$\nThe pdf of the Gaussian is given by:\n$$\\mathcal{N}(y\\mid\\mu,\\sigma^2)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}},\\exp!\\Bigl(-\\tfrac{1}{2\\sigma^2}(y-\\mu)^2\\Bigr).$$\nThe mean of the distribution is:\n$$\\mathbb{E}[\\mathcal{N}(\\cdot\\mid\\mu,\\sigma^2)]=\\mu,$$\nand the standard deviation is:\n$$std[\\mathcal{N}(\\cdot\\mid\\mu,\\sigma^2)]=\\sigma.$$\nIt is common to parameterize the Gaussian as a function of input variables to create a conditional density model of the form:\n$$p(y\\mid\\mathbf{x},\\theta)=\\mathcal{N}\\Bigl(y\\mid f_{\\mu}(\\mathbf{x};\\theta),, f_{\\sigma}(\\mathbf{x};\\theta)^2\\Bigr),$$\nwhere ${f_{\\mu}(\\mathbf{x};\\theta)\\in \\mathbb{R}}$ predicts the mean and ${f_{\\sigma}(\\mathbf{x};\\theta)\\in \\mathbb{R}_+}$ predicts the variance.\nAssuming fixed variance independent of the input (homoscedastic regression) and a linear mean, we have:\n$$p(y\\mid\\mathbf{x},\\theta)=\\mathcal{N}\\Bigl(y\\mid \\mathbf{W}^T\\mathbf{x}+b,,\\sigma^2\\Bigr)$$\nwith ${\\theta=(\\mathbf{W},b,\\sigma^2)}$. If the variance depends on the input (heteroskedastic regression), then:\n$$p(y\\mid\\mathbf{x},\\theta)=\\mathcal{N}\\Bigl(y\\mid\\mathbf{W}{\\mu}^T\\mathbf{x}+b,\\sigma+\\bigl(\\mathbf{W}_\\sigma^T\\mathbf{x}\\bigr)^2\\Bigr)$$\nwith $$\\theta=(\\mathbf{W}\\mu,\\mathbf{W}\\sigma)$$, and $\\sigma_+(x)$ being the softplus function mapping ${\\mathbb{R}}$ to ${\\mathbb{R}_+}$.\nWhen the variance approaches 0, the Gaussian becomes infinitely narrow:\n$$\\lim_{\\sigma\\rightarrow0}\\mathcal{N}(y\\mid\\mu,\\sigma^2)\\rightarrow\\delta(y-\\mu),$$\nwhere the Dirac delta function is defined as:\n$$\\delta(x) = \\begin{cases} +\\infty \u0026amp; \\text{if } x=0, \\ 0 \u0026amp; \\text{if } x \\neq 0 \\end{cases},$$\nand similarly,\n$$\\delta_y(x) = \\begin{cases} +\\infty \u0026amp; \\text{if } x=y, \\ 0 \u0026amp; \\text{if } x \\neq y \\end{cases},$$\nwith ${\\delta_y(x) = \\delta(x-y)}$.\nSome Common Other Univariate Distributions Student t Distribution $$\\mathcal{T}(y \\mid \\mu, \\sigma^2, \\nu) ;\\propto; \\Bigl[ 1 + \\tfrac{1}{\\nu} \\Bigl(\\tfrac{y - \\mu}{\\sigma}\\Bigr)^2 \\Bigr]^{-\\tfrac{\\nu + 1}{2}}.$$\nIts properties include:\n$$\\text{mean} = \\mu,\\quad \\text{mode} = \\mu,\\quad \\text{var} = \\frac{\\nu \\sigma^2}{\\nu - 2},$$\nwith the mean defined if ${\\nu \u0026gt; 1}$ and the variance if ${\\nu \u0026gt; 2}$. For ${\\nu \\gg 5}$, it approaches a Gaussian. A common choice is ${\\nu = 4}$.\nCauchy Distribution For ${\\nu=1}$, the Student t distribution becomes the Cauchy (or Lorentz) distribution:\n$$\\mathcal{C}(x\\mid\\mu,\\gamma)=\\frac{1}{\\pi,\\gamma} \\Bigl[1+\\Bigl(\\tfrac{x-\\mu}{\\gamma}\\Bigr)^2\\Bigr]^{-1}.$$\nThe Half Cauchy distribution (with ${\\mu=0}$) is defined as:\n$$\\mathcal{C}_+(x\\mid \\gamma) ;\\triangleq; \\tfrac{2}{\\pi,\\gamma} \\Bigl[1+\\Bigl(\\tfrac{x}{\\gamma}\\Bigr)^2\\Bigr]^{-1}.$$\nLaplace Distribution Also known as the double-sided exponential distribution, the Laplace distribution has the pdf:\n$$\\text{Laplace}(y \\mid \\mu, b) ;\\triangleq; \\frac{1}{2b} \\exp\\Bigl(-\\tfrac{|y-\\mu|}{b}\\Bigr),$$\nwith properties:\n$$\\text{mean} = \\mu,\\quad \\text{mode} = \\mu,\\quad \\text{var} = 2b^2.$$\nBeta Distribution The Beta distribution is supported on the interval ${[0,1]}$ and is defined as:\n$$\\text{Beta}(x \\mid a, b) ;=; \\frac{1}{B(a, b)},x^{,a-1},\\bigl(1 - x\\bigr)^{b-1},$$\nwhere the beta function is:\n$$B(a, b) ;\\triangleq; \\frac{\\Gamma(a),\\Gamma(b)}{\\Gamma(a + b)},$$\nand the Gamma function is defined as:\n$$\\Gamma(a) ;\\triangleq; \\int_{0}^{\\infty} x^{,a-1}e^{-x},dx.$$\nFor ${a = b = 1}$, this is the uniform distribution. When both ${a}$ and ${b}$ are less than 1, the distribution is bimodal with spikes at ${0}$ and ${1}$; when both are greater than 1, it is unimodal. Its properties include:\n$$\\text{mean} = \\frac{a}{,a+b,}, \\quad \\text{mode} = \\frac{a-1}{,a+b-2,}, \\quad \\text{var} = \\frac{ab}{(,a+b,)^2(,a+b+1,)}.$$\nTransformation of Random Variables Suppose ${\\mathbf{x}\\sim p()}$ is a random variable, and ${\\mathbf{y}=f(\\mathbf{x})}$ is a transformation of it. We discuss how to compute ${p(\\mathbf{y})}$.\nDiscrete Case For discrete r.v\u0026rsquo;s, the pmf of ${\\mathbf{y}}$ is obtained by summing the pmf of all ${\\mathbf{x}}$ such that ${f(\\mathbf{x})=\\mathbf{y}}$:\n$$p_{\\mathbf{y}}(\\mathbf{y})=\\sum_{\\mathbf{x}:,f(\\mathbf{x})=\\mathbf{y}}p_{\\mathbf{x}}(\\mathbf{x}).$$\nFor example, if ${f(\\mathbf{x})=1}$ when ${\\mathbf{x}}$ is even and ${0}$ otherwise, and ${\\mathbf{x}}$ is uniformly distributed over ${{1,2,\\dots,10}}$, then ${p_{\\mathbf{y}}(1)=0.5}$ and ${p_{\\mathbf{y}}(0)=0.5}$.\nContinuous Case For continuous r.v\u0026rsquo;s, we work with the cdf:\n$$P_{\\mathbf{y}}(\\mathbf{y})=Pr(Y\\leq \\mathbf{y}) = Pr\\Bigl(f(\\mathbf{x}) \\leq \\mathbf{y}\\Bigr)=Pr\\Bigl(\\mathbf{x} \\in {x \\mid f(x) \\leq \\mathbf{y}}\\Bigr).$$\nIf ${f}$ is invertible, differentiating the cdf yields the pdf; if not, Monte Carlo approximation may be used.\nInvertible Transformation (Bijections or One-to-One) For a monotonic (and hence invertible) function, if ${x\\sim Uni(0,1)}$ and ${y=f(x)=2x+1}$, then for general ${p_x(x)}$ and any monotonic ${f:\\mathbb{R}\\rightarrow\\mathbb{R}}$, let ${g=f^{-1}}$ with ${y=f(x)}$ and ${x=g(y)}$. Then:\n$$P_y(y)=Pr!\\bigl(f(X)\\leq y\\bigr)=Pr!\\bigl(X\\leq f^{-1}(y)\\bigr)=P_x!\\bigl(f^{-1}(y)\\bigr)=P_x!\\bigl(g(y)\\bigr).$$\nDifferentiating gives:\n$$p_y(y) \\triangleq \\frac{d}{dy}P_y(y)=p_x!\\bigl(g(y)\\bigr),\\Bigl|\\frac{dx}{dy}\\Bigr|.$$\nFor multivariate cases, if ${f:\\mathbb{R}^n\\rightarrow\\mathbb{R}^n}$ is invertible with inverse ${g}$, then:\n$$p_y(\\mathbf{y})=p_x(\\mathbf{x}),\\Bigl|\\det\\Bigl[\\mathbf{J}_g(\\mathbf{y})\\Bigr]\\Bigr|,$$\nwhere ${\\mathbf{J}_g(\\mathbf{y})=\\frac{d\\mathbf{g}(\\mathbf{y})}{,d\\mathbf{y}^T}}$ is the Jacobian.\nConvolution Theorem For ${Y=X_1+X_2}$ with independent r.v\u0026rsquo;s ${X_1}$ and ${X_2}$, in the discrete case:\n$$p(Y=y)=\\sum_{x=-\\infty}^{\\infty}p(X_1=x),p(X_2=y-x).$$\nIn the continuous case:\n$$p(y)=\\int p_1(x_1),p_2(y-x_1),dx_1.$$\nThis is written as:\n$$p = p_1 \\circledast p_2,$$\nwhere ${\\circledast}$ is the convolution operator.\nCentral Limit Theorem The Central Limit Theorem states that the sum of ${N}$ independent and identically distributed (i.i.d.) random variables (regardless of their original distribution) will approximate a Gaussian distribution as ${N}$ increases.\nMonte Carlo Approximation Suppose ${\\mathbf{x}}$ is a random variable and ${\\mathbf{y}=f(\\mathbf{x})}$. When computing ${p(\\mathbf{y})}$ directly is difficult, one can approximate it by drawing a large number of samples from ${p(x)}$, computing ${y_s=f(x_s)}$, and forming the empirical distribution:\n$$p_{\\mathbf{y}}(y) \\triangleq \\frac{1}{N_s} ,\\sum_{s=1}^{N_s} \\delta!\\bigl(y - y_s\\bigr).$$\n","permalink":"http://localhost:1313/posts/univariate-models/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eCalling \u003cstrong\u003esample space\u003c/strong\u003e ${\\mathcal{X}}$ are all possible experiences, and an \u003cstrong\u003eevent\u003c/strong\u003e will be a subset of the sample space.\u003c/p\u003e\n\u003ch3 id=\"union\"\u003eUnion\u003c/h3\u003e\n\u003cp\u003e$$Pr(A \\land B)=Pr(A,B).$$\u003c/p\u003e\n\u003cp\u003eIf independent events,\u003c/p\u003e\n\u003cp\u003e$$Pr(A \\land B)=Pr(A)Pr(B).$$\u003c/p\u003e\n\u003cp\u003eWe say a set of variables ${X_1, \\dots, X_n}$ is (mutually) independent if the joint can be written as a product of marginals for all subsets ${{X_1,\\dots, X_m} \\subseteq {X_1,\\dots,X_n}}$,\u003c/p\u003e\n\u003cp\u003e$$p(X_1,X_2,\\dots,X_n)=\\prod_{i=1}^{m}p(X_i).$$\u003c/p\u003e\n\u003ch3 id=\"disjoint\"\u003eDisjoint\u003c/h3\u003e\n\u003cp\u003e$$Pr(A \\vee B)=Pr(A)+Pr(B)-Pr(A\\land B).$$\u003c/p\u003e\n\u003ch3 id=\"conditional-probability\"\u003eConditional Probability\u003c/h3\u003e\n\u003cp\u003e$$Pr(B|A)\\triangleq \\frac{Pr(A,B)}{Pr(A)}.$$\u003c/p\u003e","title":"Univariate Models"}]