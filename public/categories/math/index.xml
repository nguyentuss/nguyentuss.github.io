<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Math on My New Hugo Site</title>
    <link>http://localhost:1313/categories/math/</link>
    <description>Recent content in Math on My New Hugo Site</description>
    <generator>Hugo -- 0.146.7</generator>
    <language>en</language>
    <lastBuildDate>Sun, 20 Apr 2025 13:34:38 +0700</lastBuildDate>
    <atom:link href="http://localhost:1313/categories/math/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Optimization</title>
      <link>http://localhost:1313/posts/optimization/</link>
      <pubDate>Sun, 20 Apr 2025 13:34:38 +0700</pubDate>
      <guid>http://localhost:1313/posts/optimization/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In &lt;em&gt;Machine Learning&lt;/em&gt;, the core problem is that solving the parameter estimation (model fitting), we want to find the values for a set of variable $\theta\in\Theta$, that minimized the scalar &lt;strong&gt;loss function&lt;/strong&gt; or &lt;strong&gt;cost function&lt;/strong&gt; $\mathcal{L}(\theta) \rightarrow \mathbb{R}$. This is called a &lt;strong&gt;optimization problem&lt;/strong&gt;.
$$
\theta^{opt} \in \arg\min \mathcal{L}(\theta)
$$
We will assume that the &lt;em&gt;parameter space&lt;/em&gt; is given by $\Theta \subseteq \mathbb{R}^D$, where $D$ is the number of variables being optimized over.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Statistic</title>
      <link>http://localhost:1313/posts/statistic/</link>
      <pubDate>Sun, 20 Apr 2025 12:03:54 +0700</pubDate>
      <guid>http://localhost:1313/posts/statistic/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In the section &lt;a href=&#34;https://nguyentuss.github.io/p/univariate-models/&#34;&gt;Univariate Models&lt;/a&gt; and  &lt;a href=&#34;https://nguyentuss.github.io/p/multivariate-models/&#34;&gt;Multivariate Models&lt;/a&gt;, we assumed all the parameters $\theta$ is known. In this section, we discuss how to learn these parameters from data.
The process of estimating $\theta$ from $\mathcal{D}$ is call &lt;strong&gt;model fitting&lt;/strong&gt;, or &lt;strong&gt;training&lt;/strong&gt;, and is at the heart of machine learning. There are many methods for producing such estimates, but most boil down to an optimization problem of the form.
$$
\widehat{\theta} = \arg\min_{\theta} \mathcal{L}(\theta)
$$
where $\mathcal{L(\theta)}$ is some kind of loss function or objective function. We discuss several different loss functions in this chapter. In some cases we also discuss how to solve the optimization problem in closed form. In general, however we will need to use some kind of generic optimization algorithm, which we will discuss in &lt;a href=&#34;https://nguyentuss.github.io/p/optimization/&#34;&gt;Optimization&lt;/a&gt;.
In addition to computing a &lt;strong&gt;point estimate $\widehat{\theta}$&lt;/strong&gt;. We discuss how to model our uncertainty or confidence in this estimate. In statistics, the process of quantifying uncertainty about an unknown quantity estimated from a finite sample of data is called &lt;strong&gt;inference&lt;/strong&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Numerical Method</title>
      <link>http://localhost:1313/posts/numerical-method/</link>
      <pubDate>Mon, 17 Mar 2025 13:18:12 +0700</pubDate>
      <guid>http://localhost:1313/posts/numerical-method/</guid>
      <description>&lt;h2 id=&#34;truncation-errors-and-the-taylor-series&#34;&gt;Truncation Errors and the Taylor Series&lt;/h2&gt;
&lt;p&gt;Truncation errors are those that result from using approximation in place of an exact mathematical procedure.
$$ \frac{dv}{dt} \approx \frac{\Delta v}{\Delta t} = \frac{v(t_{i+1})-v(t_i)}{t_{i+1}-t_i}$$
A truncation error was introduced into the numerical solution because the difference equation only approximates the true value of the derivative. In order to gain insight into the properties of such errors, we now turn to a mathematical formulation that is used widely in numerical methods to express functions in an approximate fashionâ€” the Taylor series.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
