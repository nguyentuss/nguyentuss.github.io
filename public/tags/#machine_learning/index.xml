<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>#Machine_Learning on My New Hugo Site</title>
    <link>https://nguyentuss.github.io/tags/%23machine_learning/</link>
    <description>Recent content in #Machine_Learning on My New Hugo Site</description>
    <generator>Hugo -- 0.146.7</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 11 Mar 2025 10:49:42 +0700</lastBuildDate>
    <atom:link href="https://nguyentuss.github.io/tags/%23machine_learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Histogram of Oriented Gradients (HOG)</title>
      <link>https://nguyentuss.github.io/posts/hog/</link>
      <pubDate>Tue, 11 Mar 2025 10:49:42 +0700</pubDate>
      <guid>https://nguyentuss.github.io/posts/hog/</guid>
      <description>&lt;hr&gt;
&lt;p&gt;There are many different methods in computer vision. When it comes to image classification, we can apply families of CNN models such as Inception Net, MobileNet, ResNet, DenseNet, AlexNet, U-Net, and so on. For object detection, models like YOLO, SSD, Faster R-CNN, Fast R-CNN, and Mask R-CNN are commonly used.
All the above algorithms belong to the deep learning category. However, before the deep learning boom, what algorithms were typically used in image processing? Today, we will explore a classic yet highly effective algorithm in image processing: &lt;strong&gt;HOG (Histogram of Oriented Gradients).&lt;/strong&gt;
This algorithm generates &lt;strong&gt;features description&lt;/strong&gt; for the purpose of &lt;strong&gt;object detection&lt;/strong&gt;. From an image, two key matrices are extracted to store essential information: &lt;strong&gt;gradient magnitude&lt;/strong&gt; and &lt;strong&gt;gradient orientation&lt;/strong&gt;. By combining these two pieces of information into a histogram distribution—where the gradient magnitude is counted in bins according to gradient orientation—we obtain a &lt;strong&gt;HOG feature vector&lt;/strong&gt; that represents the histogram. This is the basic concept, but in practice, the algorithm is more complex. The &lt;strong&gt;HOG vector&lt;/strong&gt; is computed over &lt;strong&gt;local regions&lt;/strong&gt;, similar to how CNNs operate, followed by &lt;strong&gt;local normalization&lt;/strong&gt; to standardize measurements. Finally, the overall &lt;strong&gt;HOG vector&lt;/strong&gt; is aggregated from all local vectors.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Multivariate Models</title>
      <link>https://nguyentuss.github.io/posts/multivariate-models/</link>
      <pubDate>Mon, 10 Mar 2025 21:50:35 +0700</pubDate>
      <guid>https://nguyentuss.github.io/posts/multivariate-models/</guid>
      <description>&lt;h2 id=&#34;joint-distributions-for-multiple-random-variables&#34;&gt;Joint distributions for multiple random variables&lt;/h2&gt;
&lt;h3 id=&#34;covariance&#34;&gt;Covariance&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;covariance&lt;/strong&gt; between two random variables ${X}$ and ${Y}$ measures the &lt;strong&gt;direction&lt;/strong&gt; of the &lt;strong&gt;linear relationship&lt;/strong&gt; to which ${X}$ and ${Y}$ are (linearly) related. It quantifies how the random variables change together.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;  Positive: If one increases, the other also increases.&lt;/li&gt;
&lt;li&gt;  Negative: If one increases while the other decreases.&lt;/li&gt;
&lt;li&gt;  Zero: There is no relationship between the variables.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$\textrm{Cov}[X,Y] \triangleq \mathbb{E}\Bigl[(X-\mathbb{E}[X])(Y-\mathbb{E}[Y])\Bigr] = \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y].$$&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
